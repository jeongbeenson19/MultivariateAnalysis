{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "회귀분석에서 베타 햇(coefficient estimates, 추정된 회귀계수)은 주어진 데이터를 기반으로 회귀모델의 회귀계수를 추정하는 것을 말합니다. 이 추정은 주어진 데이터에 가장 적합한 회귀선을 찾는 것으로, 일반적으로 최소제곱법을 사용하여 수행됩니다.\n",
        "\n",
        "1. **베타 햇 추정 방법**:\n",
        "    - **정규방정식(Normal Equations)**: 단순선형회귀분석의 경우, 정규방정식을 사용하여 추정된 회귀계수를 구할 수 있습니다.\n",
        "    - **경사 하강법(Gradient Descent)**: 대규모 데이터셋이나 다중선형회귀분석의 경우, 경사 하강법과 같은 최적화 알고리즘을 사용하여 회귀계수를 추정할 수 있습니다.\n",
        "    - **특이값 분해(Singular Value Decomposition, SVD)**: 특이값 분해를 이용하여 회귀분석을 수행하고 회귀계수를 추정할 수도 있습니다.\n",
        "    - **경사 하강법을 포함한 반복적인 최적화 알고리즘**: Regularized 회귀분석(릿지, 라쏘, 엘라스틱넷)에서 사용되는 알고리즘과 같이 반복적인 최적화 알고리즘을 사용하여 회귀계수를 추정할 수 있습니다.\n",
        "\n",
        "2. **추정된 회귀계수의 특징**:\n",
        "    - 추정된 회귀계수는 특정 데이터셋에 대해 추정되는 값이므로, 표본 통계량으로 해석되어야 합니다.\n",
        "    - 추정된 회귀계수의 신뢰 구간을 구할 수 있으며, 이를 통해 추정의 불확실성을 평가할 수 있습니다.\n",
        "    - 회귀분석에서 다른 변수들과의 관계를 고려하면서 회귀계수를 해석해야 합니다.\n",
        "\n",
        "따라서, 회귀분석에서 베타 햇은 주어진 데이터에 대해 최적화된 회귀모델의 회귀계수를 추정하는 것을 의미하며, 이는 여러가지 방법을 사용하여 수행될 수 있습니다."
      ],
      "metadata": {
        "id": "QCX1EiA-nLIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "인공 신경망(뉴럴 네트워크)은 생물학적 뉴런의 작동 원리를 모방한 계산 모델입니다. 각각의 뉴런은 입력 신호를 받아 가중치를 곱한 후 활성화 함수를 거쳐 출력값을 내보냅니다. 뉴럴 네트워크는 여러 층의 뉴런으로 구성되어 있으며, 각 층은 입력층, 은닉층, 출력층으로 나뉩니다.\n",
        "\n",
        "아래는 일반적인 뉴럴 네트워크의 구성 요소입니다:\n",
        "\n",
        "1. **입력층(Input Layer)**:\n",
        "   - 데이터가 입력되는 부분입니다.\n",
        "   - 입력층의 뉴런 수는 입력 데이터의 특성(feature) 수와 일치합니다.\n",
        "\n",
        "2. **은닉층(Hidden Layers)**:\n",
        "   - 입력층과 출력층 사이에 있는 중간 층입니다.\n",
        "   - 하나 이상의 은닉층이 있을 수 있습니다.\n",
        "   - 각 은닉층은 여러 개의 뉴런으로 구성되어 있습니다.\n",
        "   - 은닉층의 뉴런은 입력 신호를 받아 가중치를 곱하고 활성화 함수를 통과한 후 출력값을 계산합니다.\n",
        "\n",
        "3. **출력층(Output Layer)**:\n",
        "   - 최종 출력을 생성하는 부분입니다.\n",
        "   - 출력층의 뉴런 수는 출력의 종류에 따라 결정됩니다. 회귀 문제의 경우 하나의 뉴런을 가질 수 있고, 분류 문제의 경우 클래스 수와 일치합니다.\n",
        "   - 출력층의 각 뉴런은 입력 신호를 받아 가중치를 곱한 후 활성화 함수를 거쳐 최종 출력을 생성합니다.\n",
        "\n",
        "4. **가중치(Weights)**:\n",
        "   - 각 연결선에는 가중치가 할당되어 있습니다.\n",
        "   - 가중치는 입력값의 중요도를 나타냅니다.\n",
        "   - 학습 과정에서 최적의 가중치를 찾기 위해 역전파(backpropagation) 알고리즘이 사용됩니다.\n",
        "\n",
        "5. **활성화 함수(Activation Functions)**:\n",
        "   - 뉴럴 네트워크의 핵심적인 구성 요소 중 하나입니다.\n",
        "   - 비선형 함수로, 뉴런의 출력을 결정합니다.\n",
        "   - 대표적인 활성화 함수로는 시그모이드 함수, 렐루 함수(Rectified Linear Unit, ReLU), 하이퍼볼릭 탄젠트 함수 등이 있습니다.\n",
        "\n",
        "6. **손실 함수(Loss Function)**:\n",
        "   - 모델의 예측값과 실제값 사이의 차이를 측정하는 함수입니다.\n",
        "   - 학습 과정에서 손실 함수의 값을 최소화하도록 모델을 학습시킵니다.\n",
        "\n",
        "뉴럴 네트워크는 이러한 구성 요소들이 서로 연결되어 있으며, 학습 데이터를 통해 가중치를 조정하여 입력값에 대한 적절한 출력을 생성하도록 학습됩니다."
      ],
      "metadata": {
        "id": "dJZs8QExdQSA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ERVRQW1ydK-6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch는 파이썬 기반의 오픈 소스 머신 러닝 라이브러리로, 주로 딥 러닝 모델을 구축하고 학습하기 위해 사용됩니다. PyTorch는 텐서 연산과 그래프를 사용하여 수치 계산을 수행하며, 동적 계산 그래프(dynamic computational graph)를 통해 유연한 모델 설계를 가능하게 합니다. 다음은 PyTorch의 주요 특징과 구성 요소에 대한 설명입니다:\n",
        "\n",
        "1. **Tensor**: PyTorch의 가장 기본적인 데이터 구조입니다. Tensor는 다차원 배열로, NumPy의 ndarray와 유사한 기능을 제공합니다. GPU를 활용하여 수치 계산을 가속화할 수 있습니다.\n",
        "\n",
        "2. **Autograd**: PyTorch의 자동 미분 엔진입니다. Autograd는 계산 그래프를 기반으로하여 텐서의 연산에 대한 자동 미분을 수행하여 그라디언트(gradient)를 계산합니다. 이를 통해 역전파(backpropagation) 알고리즘을 구현할 수 있습니다.\n",
        "\n",
        "3. **Neural Network Module**: PyTorch에서 제공하는 딥 러닝 모듈입니다. `torch.nn` 모듈을 통해 다양한 종류의 뉴럴 네트워크 레이어 및 활성화 함수를 사용할 수 있습니다. 또한 사용자 정의 뉴럴 네트워크 모듈을 만들 수 있습니다.\n",
        "\n",
        "4. **Optimization**: PyTorch는 SGD(Sthochastic Gradient Descent) 및 다양한 최적화 알고리즘을 제공합니다. `torch.optim` 모듈을 사용하여 모델의 가중치를 업데이트할 수 있습니다.\n",
        "\n",
        "5. **Dataset and DataLoader**: PyTorch는 데이터셋을 로드하고 학습에 사용할 수 있는 유틸리티를 제공합니다. `torch.utils.data.Dataset` 클래스를 상속하여 사용자 정의 데이터셋을 만들고, `torch.utils.data.DataLoader`를 사용하여 미니배치(mini-batch)를 생성할 수 있습니다.\n",
        "\n",
        "6. **CUDA 지원**: PyTorch는 CUDA를 통해 GPU 가속을 지원하여 모델의 학습과 추론을 가속화할 수 있습니다.\n",
        "\n",
        "PyTorch는 사용하기 쉽고 유연하며 Pythonic한 인터페이스를 제공하여 딥 러닝 모델의 구축 및 학습을 간편하게 할 수 있습니다. 또한 활발한 커뮤니티와 풍부한 문서화가 제공되어 있어 새로운 사용자들이 쉽게 시작할 수 있습니다."
      ],
      "metadata": {
        "id": "vlioFzA3mSay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                            [93, 88, 93],\n",
        "                            [89, 91, 90],\n",
        "                            [96, 98, 100],\n",
        "                            [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35rGh_jelpmq",
        "outputId": "67dc1fa8-349d-4fad-d38c-8cc2349ebaba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3]) torch.Size([5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Linear(3, 1)"
      ],
      "metadata": {
        "id": "6Kk3eGPnrpya"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NqhLngpsOJw",
        "outputId": "fb1bef0b-a9b3-4fc9-9ec1-ca9d0201b930"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=3, out_features=1, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr =1e-5)"
      ],
      "metadata": {
        "id": "hzbD6gdFsSVv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukl4QItRtjBD",
        "outputId": "c1b3a87a-6bb2-47a1-9150-60883af9b2c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-56.3954],\n",
              "        [-69.5581],\n",
              "        [-67.5259],\n",
              "        [-74.3842],\n",
              "        [-52.8532]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\sum (\\hat{y_i} - y_i)\n",
        "$$"
      ],
      "metadata": {
        "id": "rz8FCFxrv6uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 100\n",
        "\n",
        "# 에포크(epoch) 동안 학습을 반복합니다.\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # 모델에 입력 데이터를 전달하여 예측값을 계산합니다.\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # 예측값과 실제값 사이의 평균 제곱 오차(MSE)를 계산합니다.\n",
        "    loss = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # 최적화를 수행하기 전에 기울기를 초기화합니다.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 역전파 알고리즘을 사용하여 손실에 대한 기울기를 계산합니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # 최적화 알고리즘을 사용하여 모델의 파라미터를 업데이트합니다.\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번째마다 실행 결과 출력\n",
        "    if epoch % 100 == 0:\n",
        "      print(prediction)\n",
        "      print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEbQuv2mut2b",
        "outputId": "96713255-f6ee-41e0-ef87-81e4b0c35fa5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-56.3954],\n",
            "        [-69.5581],\n",
            "        [-67.5259],\n",
            "        [-74.3842],\n",
            "        [-52.8532]], grad_fn=<AddmmBackward0>)\n",
            "tensor(56114.5742, grad_fn=<MseLossBackward0>)\n",
            "tensor([[153.7258],\n",
            "        [183.1091],\n",
            "        [181.3704],\n",
            "        [196.6643],\n",
            "        [139.8919]], grad_fn=<AddmmBackward0>)\n",
            "tensor(2.6635, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLm5_-KTg9tC",
        "outputId": "9e6373d2-351b-44a9-f25b-683dd5739121"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[0.7781, 0.8485, 0.3907]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.2653], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "test = model(torch.Tensor(np.array([[73, 80, 75]])))\n",
        "test.item()\n",
        "# tensor([[156.0935]], grad_fn=<AddmmBackward0>)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN_o6k7uhVy9",
        "outputId": "00a5e7d0-e3cd-4639-82bf-9b2f80f98d11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153.72518920898438"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch에서 모델의 파라미터를 저장하고 불러오는 방법은 다음과 같습니다:\n",
        "\n",
        "1. **모델 전체를 저장하고 불러오기**:\n",
        "   - 모델의 전체 상태를 저장하고 불러오는 가장 간단한 방법은 `torch.save()`와 `torch.load()` 함수를 사용하는 것입니다. 이 함수들은 모델의 상태를 직렬화하여 파일로 저장하고, 나중에 불러올 수 있습니다.\n",
        "   - 예를 들어, 모델의 상태를 저장하는 코드는 다음과 같습니다:\n",
        "     ```python\n",
        "     torch.save(model.state_dict(), 'model.pth')\n",
        "     ```\n",
        "     그리고 모델의 상태를 불러오는 코드는 다음과 같습니다:\n",
        "     ```python\n",
        "     model.load_state_dict(torch.load('model.pth'))\n",
        "     ```\n",
        "\n",
        "2. **특정 파라미터만 저장하고 불러오기**:\n",
        "   - 때로는 모델의 일부 파라미터만 저장하고 불러오는 것이 유용할 수 있습니다. 이를 위해 `state_dict()` 메서드를 사용하여 모델의 파라미터를 딕셔너리 형태로 얻을 수 있습니다.\n",
        "   - 예를 들어, 모델의 특정 파라미터를 저장하는 코드는 다음과 같습니다:\n",
        "     ```python\n",
        "     torch.save(model.fc1.weight, 'weights.pth')\n",
        "     ```\n",
        "     그리고 해당 파라미터를 불러오는 코드는 다음과 같습니다:\n",
        "     ```python\n",
        "     model.fc1.weight = torch.load('weights.pth')\n",
        "     ```\n",
        "\n",
        "3. **모델의 아키텍처와 파라미터를 함께 저장하기**:\n",
        "   - 모델의 아키텍처와 파라미터를 함께 저장하고 불러오려면, 모델 클래스를 정의하고 `torch.save()` 및 `torch.load()` 함수를 사용하여 모델 인스턴스를 저장하고 불러올 수 있습니다.\n",
        "   - 예를 들어, 모델을 저장하는 코드는 다음과 같습니다:\n",
        "     ```python\n",
        "     torch.save(model, 'model.pth')\n",
        "     ```\n",
        "     그리고 모델을 불러오는 코드는 다음과 같습니다:\n",
        "     ```python\n",
        "     model = torch.load('model.pth')\n",
        "     ```\n",
        "\n",
        "위 방법들을 사용하여 PyTorch 모델의 파라미터를 저장하고 불러올 수 있습니다. 선택한 방법은 사용 사례에 따라 다를 수 있으며, 각각의 장단점을 고려하여 적절히 선택해야 합니다."
      ],
      "metadata": {
        "id": "g7DX_xL4iSSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"model_name.pt\")"
      ],
      "metadata": {
        "id": "kiZiuuKphhOU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model_name.pt')"
      ],
      "metadata": {
        "id": "9rUx2dksjHhD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_1 = model(torch.Tensor(np.array([[73, 80, 75]])))\n",
        "test_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHr59FZijL_W",
        "outputId": "dfa894b3-570d-4f94-9871-9e4d87089a78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[153.7252]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch의 DataLoader는 데이터셋을 미니배치(mini-batch)로 나누어 모델에 공급하는 역할을 합니다. DataLoader는 데이터셋을 반복하고 필요에 따라 데이터를 로드하는 기능을 제공하여 모델 학습을 용이하게 합니다. DataLoader는 주로 `torch.utils.data.DataLoader` 클래스를 사용하여 생성됩니다.\n",
        "\n",
        "DataLoader의 주요 특징은 다음과 같습니다:\n",
        "\n",
        "1. **미니배치 생성**: DataLoader는 데이터셋을 미니배치로 나누어 제공합니다. 미니배치의 크기는 사용자가 지정할 수 있으며, 기본값은 1입니다.\n",
        "\n",
        "2. **데이터 셔플링**: 학습 시 데이터셋을 무작위로 섞어 모델의 학습을 안정화시키고 과적합을 방지하는 데 도움이 됩니다. DataLoader는 데이터를 셔플링하여 무작위로 미니배치를 생성할 수 있습니다.\n",
        "\n",
        "3. **병렬 처리**: DataLoader는 여러 개의 프로세스를 사용하여 데이터를 로드하여 성능을 향상시킬 수 있습니다. 이는 `num_workers` 매개변수를 통해 조절할 수 있습니다.\n",
        "\n",
        "4. **데이터 변환(Transforms)**: DataLoader는 데이터 로드 시에 변환 함수를 적용할 수 있습니다. 예를 들어, 이미지 데이터셋을 불러올 때 이미지를 텐서로 변환하거나 정규화할 수 있습니다.\n",
        "\n",
        "5. **인덱싱**: DataLoader를 이용하여 데이터셋의 특정 인덱스에 있는 데이터에 접근할 수 있습니다.\n",
        "\n",
        "다음은 DataLoader를 사용하여 데이터셋을 로드하는 예제 코드입니다:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 예시 데이터셋 생성\n",
        "x_train = torch.randn(100, 3)  # 입력 데이터\n",
        "y_train = torch.randn(100)     # 타겟 데이터\n",
        "\n",
        "# TensorDataset을 사용하여 입력 데이터와 타겟 데이터를 결합합니다.\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "# DataLoader를 사용하여 데이터셋을 로드합니다.\n",
        "batch_size = 32\n",
        "# 미니배치의 크기를 지정합니다. 하나의 미니배치에 포함되는 데이터의 샘플 수입니다.\n",
        "\n",
        "shuffle = True\n",
        "# 데이터를 셔플링할지 여부를 결정합니다. 셔플링을 통해 데이터의 순서를 무작위로 섞어 모델의 학습을 안정화시키고 과적합을 방지합니다.\n",
        "\n",
        "num_workers = 4\n",
        "# 데이터를 로드하는 데 사용할 프로세스의 수입니다. 높은 값을 사용하면 데이터 로드 속도를 높일 수 있지만, 메모리 사용량이 증가할 수 있습니다.\n",
        "# 주의: Windows에서는 num_workers를 0으로 설정해야 합니다.\n",
        "# Windows에서는 메인 프로세스(main process)에서만 DataLoader를 사용하여 데이터를 로드할 수 있습니다.\n",
        "# 따라서 Windows에서는 다중 프로세스로 데이터를 로드할 수 없으므로 num_workers를 0으로 설정해야 합니다.\n",
        "\n",
        "# DataLoader를 사용하여 데이터셋을 로드합니다.\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "\n",
        "# DataLoader를 사용하여 미니배치를 순회하며 데이터를 사용합니다.\n",
        "for inputs, targets in dataloader:\n",
        "    # 미니배치에서 입력과 타겟 데이터를 사용하여 모델을 학습하거나 추론합니다.\n",
        "    pass\n",
        "```\n",
        "\n",
        "위 코드에서는 먼저 TensorDataset을 사용하여 입력 데이터와 타겟 데이터를 결합하고, 이를 DataLoader에 전달하여 데이터셋을 로드합니다. DataLoader를 이용하여 미니배치를 순회하며 입력과 타겟 데이터를 사용하여 모델을 학습하거나 추론할 수 있습니다."
      ],
      "metadata": {
        "id": "GlH6WA0elO-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "0ka8IW_LkFBl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                            [93, 88, 93],\n",
        "                            [89, 91, 90],\n",
        "                            [96, 98, 100],\n",
        "                            [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "metadata": {
        "id": "lDz9iMXvmP9K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(x_train, y_train)\n",
        "dataset[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa2A0iOPmasP",
        "outputId": "eefbbafa-99e4-4925-eb86-dcba73f23036"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[73., 80., 75.],\n",
              "         [93., 88., 93.],\n",
              "         [89., 91., 90.]]),\n",
              " tensor([[152.],\n",
              "         [185.],\n",
              "         [180.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "0mN3c_tVnFWb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfbtO1C7oK_z",
        "outputId": "5db0d758-85ad-46a9-cde7-0f05830af364"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 89.,  91.,  90.],\n",
              "         [ 96.,  98., 100.]]),\n",
              " tensor([[180.],\n",
              "         [196.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝 모델을 클래스로 정의하는 방법은 PyTorch를 기준으로 설명하겠습니다. PyTorch를 사용하면 간단한 코드로 딥러닝 모델을 클래스로 만들 수 있습니다.\n",
        "\n",
        "아래는 딥러닝 모델을 클래스로 정의하는 기본적인 예제 코드입니다:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        # 신경망의 각 레이어를 정의합니다.\n",
        "        self.fc1 = nn.Linear(in_features=10, out_features=64)  # 입력 레이어\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=32)  # 은닉 레이어\n",
        "        self.fc3 = nn.Linear(in_features=32, out_features=1)  # 출력 레이어\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 순전파(forward) 함수를 정의합니다.\n",
        "        x = torch.relu(self.fc1(x))  # 입력을 첫 번째 레이어에 통과시키고 활성화 함수를 적용합니다.\n",
        "        x = torch.relu(self.fc2(x))  # 두 번째 레이어에 통과시키고 활성화 함수를 적용합니다.\n",
        "        x = self.fc3(x)  # 세 번째 레이어에 통과시킵니다.\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스를 생성합니다.\n",
        "model = SimpleModel()\n",
        "\n",
        "# 모델을 사용하여 예측을 수행합니다.\n",
        "inputs = torch.randn(10)  # 임의의 입력 데이터\n",
        "outputs = model(inputs)  # 모델에 입력을 전달하여 예측값을 얻습니다.\n",
        "print(outputs)\n",
        "```\n",
        "\n",
        "위 코드에서 `SimpleModel` 클래스는 `nn.Module` 클래스를 상속하고 있으며, `__init__` 메서드에서는 모델의 레이어를 정의하고, `forward` 메서드에서는 데이터의 순전파를 정의합니다.\n",
        "\n",
        "딥러닝 모델을 클래스로 정의할 때 주의할 점은 다음과 같습니다:\n",
        "\n",
        "1. `__init__` 메서드에서는 모델의 레이어를 정의하고, `forward` 메서드에서는 데이터의 순전파 과정을 정의합니다.\n",
        "2. 모델의 레이어는 `nn.Module`의 하위 클래스인 레이어들로 구성되어야 합니다.\n",
        "3. `forward` 메서드는 데이터의 순전파 과정을 정의하며, 모델을 사용하여 예측을 수행할 때 호출됩니다.\n",
        "\n",
        "이러한 구조를 따르면 모델을 쉽게 정의하고 재사용할 수 있습니다."
      ],
      "metadata": {
        "id": "xJP0pcX3ppPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Q0-tIvGfpmr-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_model = CustomModel()\n",
        "user_model.forward(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV3uWvggs_ib",
        "outputId": "ad23152c-a0c8-4ae3-dabc-452d8c8fcb26"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[40.3204],\n",
              "        [45.4333],\n",
              "        [46.4424],\n",
              "        [49.1010],\n",
              "        [35.1297]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(user_model.parameters(), lr = 0.001)\n"
      ],
      "metadata": {
        "id": "WvqCVJnFyU4_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    x_train, y_train = samples\n",
        "    prediction = user_model(x_train)\n",
        "\n",
        "  cost = criterion(prediction, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "S_3Zd13YzDky"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AfyWsi3dzWcH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}