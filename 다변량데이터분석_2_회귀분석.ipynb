{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "회귀분석에서 베타 햇(coefficient estimates, 추정된 회귀계수)은 주어진 데이터를 기반으로 회귀모델의 회귀계수를 추정하는 것을 말합니다. 이 추정은 주어진 데이터에 가장 적합한 회귀선을 찾는 것으로, 일반적으로 최소제곱법을 사용하여 수행됩니다.\n",
    "\n",
    "1. **베타 햇 추정 방법**:\n",
    "    - **정규방정식(Normal Equations)**: 단순선형회귀분석의 경우, 정규방정식을 사용하여 추정된 회귀계수를 구할 수 있습니다.\n",
    "    - **경사 하강법(Gradient Descent)**: 대규모 데이터셋이나 다중선형회귀분석의 경우, 경사 하강법과 같은 최적화 알고리즘을 사용하여 회귀계수를 추정할 수 있습니다.\n",
    "    - **특이값 분해(Singular Value Decomposition, SVD)**: 특이값 분해를 이용하여 회귀분석을 수행하고 회귀계수를 추정할 수도 있습니다.\n",
    "    - **경사 하강법을 포함한 반복적인 최적화 알고리즘**: Regularized 회귀분석(릿지, 라쏘, 엘라스틱넷)에서 사용되는 알고리즘과 같이 반복적인 최적화 알고리즘을 사용하여 회귀계수를 추정할 수 있습니다.\n",
    "\n",
    "2. **추정된 회귀계수의 특징**:\n",
    "    - 추정된 회귀계수는 특정 데이터셋에 대해 추정되는 값이므로, 표본 통계량으로 해석되어야 합니다.\n",
    "    - 추정된 회귀계수의 신뢰 구간을 구할 수 있으며, 이를 통해 추정의 불확실성을 평가할 수 있습니다.\n",
    "    - 회귀분석에서 다른 변수들과의 관계를 고려하면서 회귀계수를 해석해야 합니다.\n",
    "\n",
    "따라서, 회귀분석에서 베타 햇은 주어진 데이터에 대해 최적화된 회귀모델의 회귀계수를 추정하는 것을 의미하며, 이는 여러가지 방법을 사용하여 수행될 수 있습니다."
   ],
   "metadata": {
    "id": "QCX1EiA-nLIQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "인공 신경망(뉴럴 네트워크)은 생물학적 뉴런의 작동 원리를 모방한 계산 모델입니다. 각각의 뉴런은 입력 신호를 받아 가중치를 곱한 후 활성화 함수를 거쳐 출력값을 내보냅니다. 뉴럴 네트워크는 여러 층의 뉴런으로 구성되어 있으며, 각 층은 입력층, 은닉층, 출력층으로 나뉩니다.\n",
    "\n",
    "아래는 일반적인 뉴럴 네트워크의 구성 요소입니다:\n",
    "\n",
    "1. **입력층(Input Layer)**:\n",
    "   - 데이터가 입력되는 부분입니다.\n",
    "   - 입력층의 뉴런 수는 입력 데이터의 특성(feature) 수와 일치합니다.\n",
    "\n",
    "2. **은닉층(Hidden Layers)**:\n",
    "   - 입력층과 출력층 사이에 있는 중간 층입니다.\n",
    "   - 하나 이상의 은닉층이 있을 수 있습니다.\n",
    "   - 각 은닉층은 여러 개의 뉴런으로 구성되어 있습니다.\n",
    "   - 은닉층의 뉴런은 입력 신호를 받아 가중치를 곱하고 활성화 함수를 통과한 후 출력값을 계산합니다.\n",
    "\n",
    "3. **출력층(Output Layer)**:\n",
    "   - 최종 출력을 생성하는 부분입니다.\n",
    "   - 출력층의 뉴런 수는 출력의 종류에 따라 결정됩니다. 회귀 문제의 경우 하나의 뉴런을 가질 수 있고, 분류 문제의 경우 클래스 수와 일치합니다.\n",
    "   - 출력층의 각 뉴런은 입력 신호를 받아 가중치를 곱한 후 활성화 함수를 거쳐 최종 출력을 생성합니다.\n",
    "\n",
    "4. **가중치(Weights)**:\n",
    "   - 각 연결선에는 가중치가 할당되어 있습니다.\n",
    "   - 가중치는 입력값의 중요도를 나타냅니다.\n",
    "   - 학습 과정에서 최적의 가중치를 찾기 위해 역전파(backpropagation) 알고리즘이 사용됩니다.\n",
    "\n",
    "5. **활성화 함수(Activation Functions)**:\n",
    "   - 뉴럴 네트워크의 핵심적인 구성 요소 중 하나입니다.\n",
    "   - 비선형 함수로, 뉴런의 출력을 결정합니다.\n",
    "   - 대표적인 활성화 함수로는 시그모이드 함수, 렐루 함수(Rectified Linear Unit, ReLU), 하이퍼볼릭 탄젠트 함수 등이 있습니다.\n",
    "\n",
    "6. **손실 함수(Loss Function)**:\n",
    "   - 모델의 예측값과 실제값 사이의 차이를 측정하는 함수입니다.\n",
    "   - 학습 과정에서 손실 함수의 값을 최소화하도록 모델을 학습시킵니다.\n",
    "\n",
    "뉴럴 네트워크는 이러한 구성 요소들이 서로 연결되어 있으며, 학습 데이터를 통해 가중치를 조정하여 입력값에 대한 적절한 출력을 생성하도록 학습됩니다."
   ],
   "metadata": {
    "id": "dJZs8QExdQSA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERVRQW1ydK-6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch는 파이썬 기반의 오픈 소스 머신 러닝 라이브러리로, 주로 딥 러닝 모델을 구축하고 학습하기 위해 사용됩니다. PyTorch는 텐서 연산과 그래프를 사용하여 수치 계산을 수행하며, 동적 계산 그래프(dynamic computational graph)를 통해 유연한 모델 설계를 가능하게 합니다. 다음은 PyTorch의 주요 특징과 구성 요소에 대한 설명입니다:\n",
    "\n",
    "1. **Tensor**: PyTorch의 가장 기본적인 데이터 구조입니다. Tensor는 다차원 배열로, NumPy의 ndarray와 유사한 기능을 제공합니다. GPU를 활용하여 수치 계산을 가속화할 수 있습니다.\n",
    "\n",
    "2. **Autograd**: PyTorch의 자동 미분 엔진입니다. Autograd는 계산 그래프를 기반으로하여 텐서의 연산에 대한 자동 미분을 수행하여 그라디언트(gradient)를 계산합니다. 이를 통해 역전파(backpropagation) 알고리즘을 구현할 수 있습니다.\n",
    "\n",
    "3. **Neural Network Module**: PyTorch에서 제공하는 딥 러닝 모듈입니다. `torch.nn` 모듈을 통해 다양한 종류의 뉴럴 네트워크 레이어 및 활성화 함수를 사용할 수 있습니다. 또한 사용자 정의 뉴럴 네트워크 모듈을 만들 수 있습니다.\n",
    "\n",
    "4. **Optimization**: PyTorch는 SGD(Sthochastic Gradient Descent) 및 다양한 최적화 알고리즘을 제공합니다. `torch.optim` 모듈을 사용하여 모델의 가중치를 업데이트할 수 있습니다.\n",
    "\n",
    "5. **Dataset and DataLoader**: PyTorch는 데이터셋을 로드하고 학습에 사용할 수 있는 유틸리티를 제공합니다. `torch.utils.data.Dataset` 클래스를 상속하여 사용자 정의 데이터셋을 만들고, `torch.utils.data.DataLoader`를 사용하여 미니배치(mini-batch)를 생성할 수 있습니다.\n",
    "\n",
    "6. **CUDA 지원**: PyTorch는 CUDA를 통해 GPU 가속을 지원하여 모델의 학습과 추론을 가속화할 수 있습니다.\n",
    "\n",
    "PyTorch는 사용하기 쉽고 유연하며 Pythonic한 인터페이스를 제공하여 딥 러닝 모델의 구축 및 학습을 간편하게 할 수 있습니다. 또한 활발한 커뮤니티와 풍부한 문서화가 제공되어 있어 새로운 사용자들이 쉽게 시작할 수 있습니다."
   ],
   "metadata": {
    "id": "vlioFzA3mSay"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "print(x_train.shape, y_train.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35rGh_jelpmq",
    "outputId": "51902ea4-9bb6-4736-bb7e-2de8fe2439d2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 3]) torch.Size([5, 1])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = nn.Linear(3, 1)"
   ],
   "metadata": {
    "id": "6Kk3eGPnrpya"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NqhLngpsOJw",
    "outputId": "16e20679-33f7-4bfb-8320-4fe2ca2a2e04"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=1, bias=True)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr =1e-5)"
   ],
   "metadata": {
    "id": "hzbD6gdFsSVv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model(x_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ukl4QItRtjBD",
    "outputId": "120ba60f-3dbf-4757-934a-df0cf0ad1503"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-24.6489],\n",
       "        [-35.4766],\n",
       "        [-31.8795],\n",
       "        [-35.0074],\n",
       "        [-28.3562]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\sum (\\hat{y_i} - y_i)\n",
    "$$"
   ],
   "metadata": {
    "id": "rz8FCFxrv6uG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nb_epochs = 100\n",
    "\n",
    "# 에포크(epoch) 동안 학습을 반복합니다.\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # 모델에 입력 데이터를 전달하여 예측값을 계산합니다.\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # 예측값과 실제값 사이의 평균 제곱 오차(MSE)를 계산합니다.\n",
    "    loss = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # 최적화를 수행하기 전에 기울기를 초기화합니다.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 알고리즘을 사용하여 손실에 대한 기울기를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 최적화 알고리즘을 사용하여 모델의 파라미터를 업데이트합니다.\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번째마다 실행 결과 출력\n",
    "    if epoch % 100 == 0:\n",
    "      print(prediction)\n",
    "      print(loss)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEbQuv2mut2b",
    "outputId": "0967d1ba-8210-400f-bd66-5c79a7097808"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-24.6489],\n",
      "        [-35.4766],\n",
      "        [-31.8795],\n",
      "        [-35.0074],\n",
      "        [-28.3562]], grad_fn=<AddmmBackward0>)\n",
      "tensor(41418.6641, grad_fn=<MseLossBackward0>)\n",
      "tensor([[155.7783],\n",
      "        [181.6240],\n",
      "        [181.9085],\n",
      "        [197.8144],\n",
      "        [137.2869]], grad_fn=<AddmmBackward0>)\n",
      "tensor(10.9641, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ]
  }
 ]
}
