{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "군집분석(Clustering analysis)은 데이터셋 내의 유사한 데이터 포인트들을 하나의 그룹으로 묶는 분석 기법입니다. 이 과정에서 데이터는 여러 개의 군집(클러스터)으로 분할되며, 같은 군집 내의 데이터는 서로 비슷하고, 다른 군집의 데이터와는 차이가 큽니다. 군집분석은 주로 다음과 같은 목적으로 사용됩니다:\n",
    "\n",
    "1. **데이터 구조 파악**: 대규모 데이터셋의 내재된 구조를 이해하고 패턴을 발견하는 데 사용됩니다.\n",
    "2. **고객 세분화**: 마케팅에서 비슷한 특성을 가진 고객들을 그룹으로 나누어 맞춤형 전략을 세우는 데 사용됩니다.\n",
    "3. **이미지 분할**: 컴퓨터 비전에서 이미지 내 객체나 영역을 분할하는 데 활용됩니다.\n",
    "4. **이상치 탐지**: 데이터에서 비정상적인 패턴이나 이상치를 식별하는 데 유용합니다.\n",
    "\n",
    "군집분석에 사용되는 주요 알고리즘에는 다음과 같은 것들이 있습니다:\n",
    "\n",
    "1. **K-평균 클러스터링 (K-means Clustering)**: 데이터를 K개의 군집으로 나누고, 각 군집의 중심을 반복적으로 업데이트하면서 최적의 군집을 찾는 방법입니다.\n",
    "2. **계층적 클러스터링 (Hierarchical Clustering)**: 데이터 포인트를 계층적으로 군집화하는 방법으로, 작은 군집부터 시작해 큰 군집으로 병합하는 방법과 큰 군집을 작은 군집으로 나누는 방법이 있습니다.\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: 데이터의 밀도 기반으로 군집을 형성하는 방법으로, 밀집된 영역을 군집으로 식별하고 밀도가 낮은 영역은 이상치로 처리합니다.\n",
    "4. **가우시안 혼합 모델 (Gaussian Mixture Model, GMM)**: 데이터가 여러 가우시안 분포로 구성되어 있다고 가정하고, 각 분포를 하나의 군집으로 보는 방법입니다.\n",
    "\n",
    "군집분석은 지도학습(supervised learning)이 아닌 비지도학습(unsupervised learning) 기법에 속합니다. 이는 데이터를 사전에 레이블 없이 분석하여 군집을 형성하는 데 유용합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad562c57414c3cd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "K-평균 클러스터링(K-means Clustering)은 군집 분석에서 가장 널리 사용되는 알고리즘 중 하나로, 데이터 포인트들을 K개의 클러스터로 나누는 비지도 학습 방법입니다. 이 알고리즘은 주어진 데이터셋을 반복적으로 분석하여 각 클러스터의 중심점(센트로이드, centroid)을 갱신함으로써 클러스터의 분포를 최적화합니다.\n",
    "\n",
    "K-평균 클러스터링의 작동 과정은 다음과 같습니다:\n",
    "\n",
    "1. **초기화**: K개의 클러스터 중심점을 무작위로 선택합니다. 이 중심점들은 초기 클러스터의 중심 역할을 합니다.\n",
    "\n",
    "2. **할당**: 각 데이터 포인트를 가장 가까운 클러스터 중심점에 할당합니다. 이를 통해 각 데이터 포인트는 하나의 클러스터에 속하게 됩니다. 유클리드 거리(Euclidean distance)가 일반적으로 사용되며, 가장 가까운 중심점을 찾는 기준이 됩니다.\n",
    "\n",
    "3. **중심점 갱신**: 각 클러스터에 할당된 데이터 포인트들의 평균을 계산하여 새로운 클러스터 중심점을 설정합니다. 즉, 클러스터 내의 모든 데이터 포인트의 평균 위치가 새로운 중심점이 됩니다.\n",
    "\n",
    "4. **반복**: 할당과 갱신 과정을 중심점이 더 이상 변하지 않을 때까지 반복합니다. 중심점이 변하지 않는다는 것은 클러스터링이 수렴했다는 것을 의미합니다.\n",
    "\n",
    "5. **종료**: 클러스터링이 수렴하면 알고리즘을 종료하고 최종 클러스터를 반환합니다.\n",
    "\n",
    "### K-평균 클러스터링의 장단점\n",
    "\n",
    "**장점**:\n",
    "- **간단하고 이해하기 쉬움**: 알고리즘이 직관적이고 구현하기 쉬워서 널리 사용됩니다.\n",
    "- **빠름**: 대규모 데이터셋에서도 비교적 빠르게 동작합니다.\n",
    "- **효율적**: 대부분의 경우 적절한 클러스터 수(K)만 주어지면 효율적으로 작동합니다.\n",
    "\n",
    "**단점**:\n",
    "- **K의 선택**: 적절한 클러스터 수(K)를 미리 알아야 하는데, 이를 결정하는 것은 어려울 수 있습니다.\n",
    "- **초기 중심점 선택**: 초기 중심점의 선택에 따라 최종 결과가 달라질 수 있으며, 최적의 결과를 보장하지 못할 수 있습니다. 이를 해결하기 위해 여러 번 실행한 후 최적의 결과를 선택하는 방법이 있습니다.\n",
    "- **구형 클러스터 가정**: K-평균은 클러스터가 구형(원형)이라고 가정합니다. 복잡한 형태의 클러스터에는 잘 맞지 않을 수 있습니다.\n",
    "- **이상치 민감성**: 이상치(outlier)에 민감하여 클러스터 중심점이 왜곡될 수 있습니다.\n",
    "\n",
    "### K-평균 클러스터링의 응용\n",
    "\n",
    "K-평균 클러스터링은 다양한 분야에서 사용됩니다:\n",
    "- **고객 세분화**: 고객 데이터를 클러스터링하여 유사한 특성을 가진 고객 그룹을 식별하고, 각 그룹에 맞춘 마케팅 전략을 수립할 수 있습니다.\n",
    "- **이미지 압축**: 이미지의 색상을 클러스터링하여 색상 팔레트를 축소하고, 이를 통해 이미지 파일 크기를 줄일 수 있습니다.\n",
    "- **문서 클러스터링**: 문서 데이터를 클러스터링하여 유사한 주제나 내용을 가진 문서들을 그룹화할 수 있습니다.\n",
    "\n",
    "K-평균 클러스터링은 데이터 분석과 패턴 인식에서 매우 유용한 도구로, 데이터의 내재된 구조를 이해하고 다양한 응용 분야에서 중요한 통찰을 얻는 데 도움을 줍니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daa0e6118137adc5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "K-means algorithm(기말)\n",
    "\n",
    "K-평균 클러스터링 알고리즘은 주어진 데이터셋을 K개의 클러스터로 나누는 방법입니다. 이 알고리즘의 절차와 원리는 다음과 같습니다:\n",
    "\n",
    "### K-평균 클러스터링의 절차\n",
    "\n",
    "1. **초기화**: \n",
    "   - K개의 클러스터 중심점(centroids)을 무작위로 선택합니다.\n",
    "   \n",
    "2. **클러스터 할당**:\n",
    "   - 각 데이터 포인트를 가장 가까운 클러스터 중심점에 할당합니다. 이 과정은 유클리드 거리(Euclidean distance)를 사용하여 각 데이터 포인트와 중심점 사이의 거리를 계산하고, 가장 가까운 중심점에 해당 데이터 포인트를 할당하는 것입니다.\n",
    "   \n",
    "3. **중심점 갱신**:\n",
    "   - 각 클러스터의 중심점을 클러스터에 속한 모든 데이터 포인트의 평균 위치로 갱신합니다. 즉, 새로운 중심점은 해당 클러스터 내의 데이터 포인트들의 좌표 평균이 됩니다.\n",
    "   \n",
    "4. **반복**:\n",
    "   - 클러스터 할당과 중심점 갱신 단계를 중심점이 더 이상 변하지 않을 때까지 반복합니다. 즉, 클러스터 할당이 더 이상 바뀌지 않거나 중심점의 이동이 매우 작아질 때까지 이 과정을 계속합니다.\n",
    "\n",
    "5. **종료**:\n",
    "   - 클러스터링이 수렴하면 알고리즘을 종료하고 최종 클러스터 중심점과 각 데이터 포인트의 클러스터 할당 결과를 반환합니다.\n",
    "\n",
    "### K-평균 클러스터링의 원리\n",
    "\n",
    "- **초기화 단계**에서는 클러스터 중심점을 무작위로 선택합니다. 이 초기 선택이 최종 결과에 큰 영향을 미칠 수 있기 때문에, 다양한 초기화를 시도하거나 K-means++와 같은 개선된 초기화 방법을 사용할 수도 있습니다.\n",
    "\n",
    "- **클러스터 할당 단계**에서는 각 데이터 포인트를 가장 가까운 중심점에 할당합니다. 유클리드 거리 외에도 다른 거리 척도(예: 맨해튼 거리)를 사용할 수도 있습니다.\n",
    "\n",
    "- **중심점 갱신 단계**에서는 각 클러스터에 할당된 데이터 포인트들의 평균 위치를 계산하여 새로운 중심점을 구합니다. 이 과정은 클러스터의 중심이 데이터의 분포 중심에 위치하도록 합니다.\n",
    "\n",
    "- 이 두 단계를 반복하면, 클러스터 중심점들이 점차 데이터 분포의 중심으로 이동하게 됩니다. 이 과정은 결국 중심점이 안정되고 데이터 포인트들의 클러스터 할당이 더 이상 바뀌지 않는 상태로 수렴하게 됩니다.\n",
    "\n",
    "### K-평균 클러스터링의 수식\n",
    "\n",
    "- **거리 계산**: \n",
    "  $$\n",
    "  d(x_i, c_j) = \\sqrt{(x_i - c_j)^2}\n",
    "  $$\n",
    "  여기서 $ x_i $는 데이터 포인트, $ c_j $는 클러스터 중심점입니다.\n",
    "\n",
    "- **중심점 갱신**: \n",
    "  $$\n",
    "  c_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i\n",
    "  $$\n",
    "  여기서 $ C_j $는 클러스터 $ j $에 속한 데이터 포인트들의 집합입니다.\n",
    "\n",
    "### 알고리즘의 장단점\n",
    "\n",
    "**장점**:\n",
    "- **간단하고 직관적**: 이해하기 쉽고 구현이 간단합니다.\n",
    "- **빠르고 효율적**: 대규모 데이터셋에서도 비교적 빠르게 동작합니다.\n",
    "\n",
    "**단점**:\n",
    "- **K의 선택**: 클러스터 수 K를 미리 정해야 하며, 적절한 K를 찾는 것이 어렵습니다.\n",
    "- **초기 중심점 선택**: 초기 중심점의 선택에 따라 결과가 달라질 수 있습니다. 이를 개선하기 위해 여러 번 실행하여 최적의 결과를 선택하는 방법이 있습니다.\n",
    "- **구형 클러스터 가정**: K-평균은 클러스터가 구형(원형)이라고 가정합니다. 복잡한 형태의 클러스터에는 잘 맞지 않을 수 있습니다.\n",
    "- **이상치 민감성**: 이상치(outlier)에 민감하여 클러스터 중심점이 왜곡될 수 있습니다.\n",
    "\n",
    "### 응용 분야\n",
    "\n",
    "K-평균 클러스터링은 다음과 같은 다양한 응용 분야에서 사용됩니다:\n",
    "- **고객 세분화**: 고객 데이터를 그룹으로 나누어 유사한 특성을 가진 고객 집단을 식별합니다.\n",
    "- **이미지 처리**: 이미지의 색상을 클러스터링하여 색상 팔레트를 축소하고 이미지 파일 크기를 줄입니다.\n",
    "- **문서 클러스터링**: 문서 데이터를 주제별로 그룹화하여 유사한 내용을 가진 문서들을 묶습니다.\n",
    "\n",
    "K-평균 클러스터링은 데이터의 내재된 구조를 이해하고 다양한 응용 분야에서 중요한 통찰을 제공하는 데 매우 유용한 도구입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37bf3ae07b895240"
  },
  {
   "cell_type": "markdown",
   "source": [
    "가우시안 혼합 모델(Gaussian Mixture Model, GMM)은 데이터를 여러 개의 가우시안 분포(Gaussian distribution, 또는 정규 분포)로 모델링하는 확률 모델입니다. 이는 군집 분석에 자주 사용되며, 데이터가 여러 개의 가우시안 분포의 혼합으로부터 생성되었다고 가정합니다.\n",
    "\n",
    "### GMM의 개념과 원리\n",
    "\n",
    "GMM은 다음과 같은 가정에 기반합니다:\n",
    "1. **혼합 모델**: 전체 데이터셋이 여러 개의 가우시안 분포로 구성된 혼합 모델로 표현됩니다.\n",
    "2. **확률 밀도 함수**: 각 데이터 포인트가 특정 가우시안 분포로부터 생성될 확률을 가집니다.\n",
    "3. **가우시안 분포**: 각 가우시안 분포는 평균(μ)과 공분산 행렬(Σ)로 정의됩니다.\n",
    "\n",
    "혼합 모델의 확률 밀도 함수는 다음과 같이 표현됩니다:\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "여기서 $ K $는 가우시안 분포의 수, $ \\pi_k $는 혼합 계수(각 가우시안 분포에 대한 가중치), $ \\mathcal{N}(x | \\mu_k, \\Sigma_k) $는 평균이 $ \\mu_k $이고 공분산 행렬이 $ \\Sigma_k $인 가우시안 분포를 의미합니다. 혼합 계수 $\\pi_k$는 0에서 1 사이의 값을 가지며, 모든 혼합 계수의 합은 1입니다:\n",
    "$$\n",
    "\\sum_{k=1}^{K} \\pi_k = 1\n",
    "$$\n",
    "\n",
    "### GMM의 학습 과정\n",
    "\n",
    "GMM의 학습은 주어진 데이터셋에 가장 잘 맞는 모델 파라미터(μ, Σ, π)를 찾는 과정입니다. 이를 위해 일반적으로 기대 최대화 알고리즘(Expectation-Maximization, EM)을 사용합니다.\n",
    "\n",
    "1. **초기화**: 파라미터 $\\mu_k, \\Sigma_k, \\pi_k$를 초기화합니다. 초기화 방법에는 K-평균 클러스터링 결과를 사용하는 것이 일반적입니다.\n",
    "  \n",
    "2. **E-단계 (Expectation step)**: 각 데이터 포인트 $x_i$가 각 가우시안 분포 $k$에 속할 확률(책임도, responsibility)을 계산합니다.\n",
    "   $$\n",
    "    \\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)}\n",
    "   $$\n",
    "\n",
    "3. **M-단계 (Maximization step)**: 책임도를 기반으로 파라미터 $\\mu_k, \\Sigma_k, \\pi_k$를 갱신합니다.\n",
    "   - 새로운 평균:\n",
    "   $$\n",
    "   \\mu_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik} x_i}{\\sum_{i=1}^{N} \\gamma_{ik}}\n",
    "   $$\n",
    "   - 새로운 공분산 행렬:\n",
    "   $$\n",
    "    \\Sigma_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T}{\\sum_{i=1}^{N} \\gamma_{ik}}\n",
    "   $$\n",
    "   - 새로운 혼합 계수:\n",
    "   $$\n",
    "   \\pi_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik}}{N}\n",
    "   $$\n",
    "\n",
    "4. **반복**: E-단계와 M-단계를 중심점이 더 이상 변하지 않을 때까지 반복합니다. 이 과정은 로그 가능도(log-likelihood)가 수렴할 때까지 계속됩니다.\n",
    "\n",
    "### GMM의 장단점\n",
    "\n",
    "**장점**:\n",
    "- **복잡한 분포 모델링 가능**: GMM은 K-평균 클러스터링보다 더 복잡하고 다양한 데이터 분포를 모델링할 수 있습니다.\n",
    "- **확률적 모델**: 각 데이터 포인트가 각 클러스터에 속할 확률을 제공하므로, 불확실성을 모델링할 수 있습니다.\n",
    "- **타원형 클러스터**: GMM은 타원형 클러스터를 모델링할 수 있어 K-평균 클러스터링의 구형 클러스터 제한을 극복합니다.\n",
    "\n",
    "**단점**:\n",
    "- **모델 파라미터 초기화**: 초기 파라미터 설정에 따라 결과가 달라질 수 있습니다.\n",
    "- **컴퓨팅 복잡성**: EM 알고리즘은 계산 비용이 높을 수 있으며, 수렴 속도가 느릴 수 있습니다.\n",
    "- **과적합**: 데이터에 너무 많은 가우시안 분포를 사용하면 모델이 과적합될 수 있습니다.\n",
    "\n",
    "### 응용 분야\n",
    "\n",
    "GMM은 다양한 응용 분야에서 사용됩니다:\n",
    "- **클러스터링**: 복잡한 데이터 구조를 가진 클러스터를 식별하는 데 유용합니다.\n",
    "- **이상치 탐지**: 정상 데이터와 이상 데이터를 구분하는 데 사용됩니다.\n",
    "- **배경 모델링**: 컴퓨터 비전에서 배경을 모델링하여 움직이는 객체를 탐지하는 데 사용됩니다.\n",
    "- **스피커 인식**: 음성 신호에서 다양한 화자의 음성을 클러스터링하여 인식하는 데 사용됩니다.\n",
    "\n",
    "GMM은 데이터의 복잡한 분포를 잘 모델링할 수 있어, 다양한 분야에서 강력한 도구로 활용되고 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d85f2ddee3ca49e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8cd9fc1fad5ea6d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "단변량 정규분포\n",
    "\n",
    "$$\n",
    "f(x : \\mu, \\sigma^2)\n",
    "$$\n",
    "= 대충 확률밀도 함수\n",
    "\n",
    "1번 MLE(Maximum Likelihood Estimation)로 평균\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = log\\prod f(x_i : \\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "2번\n",
    "$$\n",
    "\\log p(x;\\theta) \\\\\n",
    "= \\log \\sum_z p(x, z;\\theta) \\\\\n",
    "= \\log \\sum_z p(x|z, \\theta) p(z|\\theta) \\\\\n",
    "= \\log E_z[p(x|z;\\theta)] \\\\\n",
    ">= E_z[log p(x|z;\\theta)] \\\\\n",
    "p(x|z;\\theta) -> c \\\\\n",
    ">\n",
    "\n",
    "$$\n",
    "\n",
    "조건부확률\n",
    "\n",
    "$$\n",
    "p(x|y) = p(x,y) / p(y)\n",
    "$$\n",
    "\n",
    "Jensen's Equality\n",
    "\n",
    "Jensen's 불평등(Jensen's inequality)은 볼록 함수(convex function)에 대한 중요한 불평등으로, 확률 이론과 통계학에서 자주 사용됩니다. 이 불평등은 기대값과 함수의 관계를 설명하는 데 유용합니다.\n",
    "\n",
    "### 정의\n",
    "\n",
    "Jensen's 불평등은 다음과 같이 정의됩니다:\n",
    "\n",
    "- \\( f \\)가 볼록 함수(convex function)이고, \\( X \\)가 확률 변수일 때,\n",
    "\\[ f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)] \\]\n",
    "여기서 \\(\\mathbb{E}[X]\\)는 \\(X\\)의 기대값을 나타냅니다.\n",
    "\n",
    "반대로, \\( f \\)가 오목 함수(concave function)인 경우에는 다음과 같이 불평등이 반대가 됩니다:\n",
    "\\[ f(\\mathbb{E}[X]) \\geq \\mathbb{E}[f(X)] \\]\n",
    "\n",
    "### 직관적 설명\n",
    "\n",
    "Jensen's 불평등은 \"함수 \\( f \\)를 기대값에 적용한 값은 함수 \\( f \\)를 각 개별 값에 적용한 후의 기대값보다 작거나 같다\"는 의미입니다. 이는 \\( f \\)가 볼록일 때 참입니다. 다시 말해, 볼록 함수는 무작위 변수의 평균보다 개별 값에 함수가 적용된 후의 평균이 더 큽니다.\n",
    "\n",
    "### 예제\n",
    "\n",
    "1. **볼록 함수 예제**:\n",
    "   - 함수 \\( f(x) = x^2 \\)는 볼록 함수입니다.\n",
    "   - \\( X \\)가 확률 변수일 때, Jensen's 불평등에 따르면:\n",
    "     \\[ (\\mathbb{E}[X])^2 \\leq \\mathbb{E}[X^2] \\]\n",
    "\n",
    "2. **오목 함수 예제**:\n",
    "   - 함수 \\( g(x) = \\sqrt{x} \\)는 오목 함수입니다.\n",
    "   - \\( X \\)가 확률 변수일 때, Jensen's 불평등에 따르면:\n",
    "     \\[ \\sqrt{\\mathbb{E}[X]} \\geq \\mathbb{E}[\\sqrt{X}] \\]\n",
    "\n",
    "### 응용\n",
    "\n",
    "Jensen's 불평등은 다음과 같은 다양한 분야에서 응용됩니다:\n",
    "- **정보 이론**: 엔트로피와 상호 정보량 계산에 사용됩니다.\n",
    "- **경제학**: 효용 함수와 기대 효용 이론에서 사용됩니다.\n",
    "- **통계학**: 여러 분포의 성질을 증명하는 데 사용됩니다.\n",
    "- **확률론**: 확률 변수의 다양한 특성을 분석하는 데 사용됩니다.\n",
    "\n",
    "### 증명 개요\n",
    "\n",
    "Jensen's 불평등의 증명은 일반적으로 볼록 함수의 정의를 사용하여 이루어집니다. 볼록 함수 \\( f \\)는 두 점 사이의 직선이 함수 그래프 위에 놓이는 특성을 가지며, 이를 수학적으로 다음과 같이 표현할 수 있습니다:\n",
    "\n",
    "- 임의의 \\( \\lambda \\in [0, 1] \\)와 두 점 \\( x \\)와 \\( y \\)에 대해,\n",
    "\\[ f(\\lambda x + (1 - \\lambda) y) \\leq \\lambda f(x) + (1 - \\lambda) f(y) \\]\n",
    "\n",
    "이 정의를 기대값을 포함하는 상황으로 확장하면 Jensen's 불평등이 도출됩니다.\n",
    "\n",
    "### 결론\n",
    "\n",
    "Jensen's 불평등은 확률 변수와 볼록 함수의 기대값 관계를 설명하는 강력한 도구입니다. 이를 통해 다양한 수학적, 통계적 성질을 이해하고 증명할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9b22c7cfefb9e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6258ec7f75f3ebad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "EM 알고리즘(Expectation-Maximization algorithm)은 불완전한 데이터 또는 숨겨진 변수(latent variables)가 있는 통계 모델의 파라미터를 추정하는 반복적인 최적화 기법입니다. 이 알고리즘은 특히 가우시안 혼합 모델(GMM)과 같은 혼합 모델의 파라미터 추정에 자주 사용됩니다.\n",
    "\n",
    "### EM 알고리즘의 개념과 절차\n",
    "\n",
    "EM 알고리즘은 두 단계로 나뉩니다: 기대 단계(Expectation step, E-step)와 최대화 단계(Maximization step, M-step). 이 두 단계를 반복하여 파라미터를 갱신하며, 반복이 수렴할 때까지 계속됩니다.\n",
    "\n",
    "1. **초기화**:\n",
    "   - 모델의 초기 파라미터를 설정합니다. 예를 들어, GMM의 경우에는 각 가우시안 분포의 평균(μ), 공분산(Σ), 혼합 계수(π)를 초기화합니다.\n",
    "\n",
    "2. **E-단계 (Expectation step)**:\n",
    "   - 현재 파라미터 값을 사용하여 숨겨진 변수의 기대값을 계산합니다. 구체적으로는, 각 데이터 포인트가 특정 클러스터(또는 가우시안 분포)에 속할 확률을 계산합니다.\n",
    "   - GMM의 경우, 각 데이터 포인트 \\(x_i\\)가 클러스터 \\(k\\)에 속할 책임도(responsibility) \\(\\gamma_{ik}\\)는 다음과 같이 계산됩니다:\n",
    "     $$\n",
    "     \\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)}\n",
    "     $$\n",
    "   \n",
    "    여기서 $\\mathcal{N}(x_i | \\mu_k, \\Sigma_k)$는 가우시안 분포의 확률 밀도 함수입니다.\n",
    "\n",
    "3. **M-단계 (Maximization step)**:\n",
    "   - E-단계에서 계산된 책임도를 사용하여 모델 파라미터를 갱신합니다. 새로운 파라미터 값은 숨겨진 변수를 최대화하도록 계산됩니다.\n",
    "   - GMM의 경우, 파라미터는 다음과 같이 갱신됩니다:\n",
    "     - 새로운 평균:\n",
    "       $$\n",
    "       \\mu_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik} x_i}{\\sum_{i=1}^{N} \\gamma_{ik}}\n",
    "       $$\n",
    "     - 새로운 공분산 행렬:\n",
    "       $$\n",
    "       \\Sigma_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T}{\\sum_{i=1}^{N} \\gamma_{ik}}\n",
    "       $$\n",
    "     - 새로운 혼합 계수:\n",
    "       $$\n",
    "       \\pi_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik}}{N}\n",
    "       $$\n",
    "\n",
    "4. **반복**:\n",
    "   - E-단계와 M-단계를 반복합니다. 각 반복에서 로그 가능도(log-likelihood)가 증가하고, 로그 가능도가 수렴하면 알고리즘을 종료합니다.\n",
    "\n",
    "### EM 알고리즘의 장단점\n",
    "\n",
    "**장점**:\n",
    "- **일반성**: EM 알고리즘은 다양한 통계 모델에 적용될 수 있으며, 데이터가 부분적으로 관측되었거나 숨겨진 변수가 있는 경우에 특히 유용합니다.\n",
    "- **수렴성**: 반복할수록 로그 가능도가 증가하며, 최적화 과정이 수렴합니다.\n",
    "- **유연성**: 복잡한 모델의 파라미터를 추정하는 데 사용될 수 있습니다.\n",
    "\n",
    "**단점**:\n",
    "- **초기화 의존성**: 초기 파라미터 설정에 따라 최종 결과가 크게 달라질 수 있습니다. 잘못된 초기화는 지역 최적해(local optimum)에 빠질 수 있습니다.\n",
    "- **수렴 속도**: 수렴 속도가 느릴 수 있으며, 특히 데이터가 많거나 모델이 복잡한 경우에는 계산 비용이 많이 듭니다.\n",
    "- **글로벌 최적화 보장 없음**: EM 알고리즘은 전역 최적해(global optimum)를 보장하지 않으며, 종종 지역 최적해에 수렴합니다.\n",
    "\n",
    "### EM 알고리즘의 응용\n",
    "\n",
    "EM 알고리즘은 다양한 분야에서 사용됩니다:\n",
    "- **가우시안 혼합 모델(GMM)**: 데이터의 군집을 찾고 각 군집의 가우시안 분포 파라미터를 추정하는 데 사용됩니다.\n",
    "- **이미지 처리**: 이미지의 객체 분할이나 복원에 사용됩니다.\n",
    "- **자연어 처리**: 숨겨진 마르코프 모델(Hidden Markov Model, HMM)의 파라미터 추정에 사용됩니다.\n",
    "- **생물정보학**: 유전자 발현 데이터 분석 및 서열 정렬에 사용됩니다.\n",
    "\n",
    "EM 알고리즘은 데이터가 불완전하거나 일부 데이터가 숨겨져 있는 상황에서 모델 파라미터를 추정하는 데 강력한 도구로, 다양한 분야에서 중요한 역할을 하고 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46a5aa304b92a6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "K-means 알고리즘은 주어진 데이터를 \\( k \\)개의 클러스터로 나누는 비지도 학습 알고리즘입니다. 주어진 수식은 K-means 알고리즘의 클러스터 할당과 클러스터 중심 업데이트 단계에 대한 수식을 설명합니다.\n",
    "\n",
    "### 1. 클러스터 할당 (Cluster Assignment)\n",
    "$$\n",
    "\\gamma_{nk} = \\arg\\min_k \\| x_n - \\mu_k \\|^2\n",
    "$$\n",
    "\n",
    "여기서 \\( \\gamma_{nk} \\)는 데이터 포인트 \\( x_n \\)이 클러스터 \\( k \\)에 속하는지 여부를 나타내는 지표입니다. 이는 다음과 같은 의미를 가집니다:\n",
    "- \\( \\gamma_{nk} = 1 \\)이면 데이터 포인트 \\( x_n \\)이 클러스터 \\( k \\)에 속합니다.\n",
    "- \\( \\gamma_{nk} = 0 \\)이면 데이터 포인트 \\( x_n \\)이 클러스터 \\( k \\)에 속하지 않습니다.\n",
    "\n",
    "이 수식은 각 데이터 포인트 \\( x_n \\)이 가장 가까운 클러스터 중심 \\( \\mu_k \\)에 할당되도록 합니다. 즉, 각 데이터 포인트는 자신과의 유클리드 거리가 최소가 되는 클러스터에 할당됩니다.\n",
    "\n",
    "### 2. 클러스터 중심 업데이트 (Centroid Update)\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum \\gamma_{nk} x_n}{\\sum \\gamma_{nk}}\n",
    "$$\n",
    "\n",
    "여기서 \\( \\mu_k \\)는 클러스터 \\( k \\)의 중심(즉, 평균)을 나타냅니다. 이 수식은 다음과 같은 과정을 통해 클러스터 중심을 업데이트합니다:\n",
    "- 클러스터 \\( k \\)에 속하는 모든 데이터 포인트 \\( x_n \\)을 더합니다. 이는 \\( \\gamma_{nk} \\)가 1인 경우에만 해당합니다.\n",
    "- 그 합을 클러스터 \\( k \\)에 속하는 데이터 포인트의 수로 나눕니다.\n",
    "\n",
    "이 과정은 클러스터 중심이 해당 클러스터에 속하는 데이터 포인트의 평균이 되도록 합니다.\n",
    "\n",
    "### 3. K-means 알고리즘 요약\n",
    "\n",
    "K-means 알고리즘은 다음과 같은 단계로 작동합니다:\n",
    "\n",
    "1. **초기화**: \\( k \\)개의 클러스터 중심 \\( \\mu_1, \\mu_2, \\ldots, \\mu_k \\)를 초기화합니다. 이는 임의의 데이터 포인트를 선택하거나 랜덤하게 설정할 수 있습니다.\n",
    "2. **클러스터 할당 (Assignment Step)**: 각 데이터 포인트 \\( x_n \\)을 가장 가까운 클러스터 중심 \\( \\mu_k \\)에 할당합니다. 이는 다음과 같은 방식으로 이루어집니다:\n",
    "   $$\n",
    "   \\gamma_{nk} = \\arg\\min_k \\| x_n - \\mu_k \\|^2\n",
    "   $$\n",
    "3. **클러스터 중심 업데이트 (Update Step)**: 각 클러스터 \\( k \\)의 중심을 해당 클러스터에 할당된 데이터 포인트의 평균으로 업데이트합니다:\n",
    "   $$\n",
    "   \\mu_k = \\frac{\\sum_{n} \\gamma_{nk} x_n}{\\sum_{n} \\gamma_{nk}}\n",
    "   $$\n",
    "4. **수렴 확인**: 클러스터 할당이 더 이상 변하지 않거나 클러스터 중심의 변화가 매우 작아질 때까지 할당 단계와 업데이트 단계를 반복합니다.\n",
    "\n",
    "### 추가적인 설명\n",
    "\n",
    "#### 초기화 방법\n",
    "K-means 알고리즘의 초기 클러스터 중심 선택은 결과에 큰 영향을 미칠 수 있습니다. 잘못된 초기화는 수렴 속도를 늦추거나 전역 최적해 대신 국소 최적해에 수렴하게 할 수 있습니다. 이를 해결하기 위해 K-means++와 같은 초기화 방법을 사용할 수 있습니다. K-means++는 다음과 같은 단계를 따릅니다:\n",
    "1. 첫 번째 클러스터 중심을 데이터 포인트 중에서 무작위로 선택합니다.\n",
    "2. 나머지 클러스터 중심은 현재 클러스터 중심과의 거리가 먼 데이터 포인트를 우선적으로 선택합니다.\n",
    "\n",
    "#### 알고리즘의 수렴\n",
    "K-means 알고리즘은 유한한 반복 후에 수렴합니다. 이는 각 반복에서 클러스터 할당과 중심 업데이트 단계가 목표 함수 \\( J \\)를 감소시키기 때문입니다. 목표 함수 \\( J \\)는 데이터 포인트와 클러스터 중심 사이의 거리의 제곱합이므로, 이 값은 항상 0 이상입니다. 따라서 알고리즘은 몇 번의 반복 후에 수렴하게 됩니다.\n",
    "\n",
    "#### 한계와 개선 방법\n",
    "- **클러스터 수 \\( k \\)**: K-means 알고리즘은 클러스터 수 \\( k \\)를 미리 알고 있어야 합니다. 이를 해결하기 위해 엘보우 방법, 실루엣 분석 등을 사용하여 적절한 \\( k \\)를 선택할 수 있습니다.\n",
    "- **초기화 민감성**: 잘못된 초기화는 나쁜 결과를 초래할 수 있습니다. K-means++ 초기화 방법은 이를 개선할 수 있습니다.\n",
    "- **구형 클러스터 가정**: K-means는 유클리드 거리를 사용하므로, 클러스터가 구형인 경우에 잘 작동합니다. 비구형 클러스터에 대해서는 Gaussian Mixture Model (GMM) 등의 알고리즘을 사용할 수 있습니다.\n",
    "- **노이즈와 이상치 민감성**: 이상치가 클러스터 중심에 큰 영향을 미칠 수 있습니다. 이를 완화하기 위해 K-medoids와 같은 변형된 알고리즘을 사용할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9266102439279ae9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "220c4c58dd7dddc6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
