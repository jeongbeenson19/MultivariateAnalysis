{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "확률밀도함수 $f(x)$에 자연로그를 취한 후 전개하는 과정과 판별함수의 수식을 제대로 설명해보겠습니다.\n",
    "\n",
    "1. **정규분포 확률밀도함수 (PDF)**\n",
    "\n",
    "   정규분포의 확률밀도함수 $ f(x) $는 다음과 같습니다:\n",
    "\n",
    "   $$\n",
    "   f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "   $$\n",
    "\n",
    "2. **자연로그 취하기**\n",
    "\n",
    "   자연로그를 취한 후 전개해보겠습니다:\n",
    "\n",
    "   $$\n",
    "   \\log(f(x)) = \\log\\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\right)\n",
    "   $$\n",
    "\n",
    "   로그의 성질을 이용하여 다음과 같이 나눕니다:\n",
    "\n",
    "   $$\n",
    "   \\log(f(x)) = \\log\\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\right) + \\log\\left( e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\right)\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\log(f(x)) = \\log\\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\right) + \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n",
    "   $$\n",
    "\n",
    "   여기서 첫 번째 항은 상수이므로 다음과 같이 씁니다:\n",
    "\n",
    "   $$\n",
    "   \\log\\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\right) = -\\log(\\sigma \\sqrt{2\\pi}) = -\\log(\\sigma) - \\log(\\sqrt{2\\pi}) = -\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi)\n",
    "   $$\n",
    "\n",
    "   따라서 최종적으로 로그 변환된 확률밀도함수는 다음과 같습니다:\n",
    "\n",
    "   $$\n",
    "   \\log(f(x)) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(x - \\mu)^2}{2\\sigma^2}\n",
    "   $$\n",
    "\n",
    "3. **판별함수**\n",
    "\n",
    "   두 개의 정규분포 $f_1(x)$와 $f_2(x)$에 대해 판별함수 $D(x)$를 정의합니다. 두 정규분포의 비율을 고려하여 결정 경계를 찾기 위해, 다음과 같은 식을 사용합니다:\n",
    "\n",
    "   $$\n",
    "   D(x) = \\log\\left( \\frac{f_1(x)}{f_2(x)} \\right) = \\log(f_1(x)) - \\log(f_2(x))\n",
    "   $$\n",
    "\n",
    "   이제 $f_1(x)$와 $f_2(x)$에 대해 로그를 취한 식을 대입합니다. 여기서 $f_1(x)$와 $f_2(x)$는 각각 $(\\mu_1, \\sigma_1)$와 $(\\mu_2, \\sigma_2)$의 매개변수를 가지는 정규분포입니다.\n",
    "\n",
    "   $$\n",
    "   \\log(f_1(x)) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma_1) - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\log(f_2(x)) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma_2) - \\frac{(x - \\mu_2)^2}{2\\sigma_2^2}\n",
    "   $$\n",
    "\n",
    "   따라서, 판별함수 $D(x)$는 다음과 같이 정의됩니다:\n",
    "\n",
    "   $$\n",
    "   D(x) = \\left( -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma_1) - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2} \\right) - \\left( -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma_2) - \\frac{(x - \\mu_2)^2}{2\\sigma_2^2} \\right)\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   D(x) = - \\log(\\sigma_1) + \\log(\\sigma_2) - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2} + \\frac{(x - \\mu_2)^2}{2\\sigma_2^2}\n",
    "   $$\n",
    "\n",
    "   이 식에서 $D(x) = 0$인 경우를 찾으면, 이 점이 두 클래스의 결정 경계를 형성합니다. 즉,\n",
    "\n",
    "   $$\n",
    "   - \\log(\\sigma_1) + \\log(\\sigma_2) - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2} + \\frac{(x - \\mu_2)^2}{2\\sigma_2^2} = 0\n",
    "   $$\n",
    "\n",
    "   이 방정식을 풀어 $x$를 구하면 결정 경계가 됩니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0851d56ff43dc4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "주어진 문제에서 정방행렬의 대각화와 주성분 벡터의 개념을 다루고 있습니다. 아래에서 각 과정을 자세히 설명하겠습니다.\n",
    "\n",
    "### 1. 대각화 (Eigenvalue Decomposition)\n",
    "\n",
    "**정의:**\n",
    "행렬 $A$를 대각화하는 것은 $A$를 고유벡터와 고유값을 이용해 대각행렬의 형태로 표현하는 과정입니다. 정방행렬 $A$에 대해, $A$를 다음과 같이 표현할 수 있습니다:\n",
    "\n",
    "$$\n",
    "A = V \\Lambda V^{-1}\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "- $V$는 $A$의 고유벡터들로 이루어진 행렬입니다.\n",
    "- $\\Lambda$는 $A$의 고유값들로 이루어진 대각행렬입니다.\n",
    "\n",
    "**고유값과 고유벡터:**\n",
    "행렬 $A$의 고유값 $\\lambda$와 고유벡터 $v$는 다음 관계를 만족합니다:\n",
    "\n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "\n",
    "이를 여러 고유값과 고유벡터로 확장하면:\n",
    "\n",
    "$$\n",
    "A \\begin{bmatrix} v_1 & v_2 \\end{bmatrix} = \\begin{bmatrix} \\lambda_1 v_1 & \\lambda_2 v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "따라서:\n",
    "\n",
    "$$\n",
    "A = V \\Lambda V^{-1}\n",
    "$$\n",
    "\n",
    "여기서 $V = \\begin{bmatrix} v_1 & v_2 \\end{bmatrix}$, $\\Lambda = \\begin{bmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bmatrix}$입니다.\n",
    "\n",
    "### 2. 주성분 벡터 (Principal Component Analysis, PCA)\n",
    "\n",
    "PCA는 데이터의 분산을 최대화하는 방향(주성분)을 찾는 기법입니다. 주성분 벡터는 데이터의 가장 큰 분산을 설명하는 방향벡터입니다. 이를 찾기 위해 데이터 행렬 $X$의 분산-공분산 행렬을 이용합니다.\n",
    "\n",
    "**목표:**\n",
    "데이터 $x$의 분산을 최대화하는 방향 $u$를 찾는 것.\n",
    "\n",
    "**오차 벡터 $e$**:\n",
    "\n",
    "$$\n",
    "\\|e\\|^2 = \\|x\\|^2 - \\|x \\cdot u\\|^2\n",
    "$$\n",
    "\n",
    "이는 데이터 $x$와 주성분 $u$ 사이의 잔차를 최소화하려는 목적을 나타냅니다.\n",
    "\n",
    "**최소화 문제를 최대화 문제로 변환:**\n",
    "\n",
    "$$\n",
    "\\|e\\|^2 = \\|x\\|^2 - (x u)^T (x u)\n",
    "$$\n",
    "\n",
    "**최소화 문제:**\n",
    "\n",
    "$$\n",
    "\\min \\|e\\|^2 = \\|x\\|^2 - u^T x^T x u\n",
    "$$\n",
    "\n",
    "이는 $x$의 분산을 $u$ 방향으로 투영한 값으로 나타냅니다.\n",
    "\n",
    "**최대화 문제:**\n",
    "\n",
    "$$\n",
    "\\max u^T x^T x u\n",
    "$$\n",
    "\n",
    "데이터 $x$의 분산을 $u$ 방향으로 최대화하려는 목적입니다.\n",
    "\n",
    "**제약 조건:**\n",
    "\n",
    "$$\n",
    "\\text{s. t. } u^T u = 1\n",
    "$$\n",
    "\n",
    "주성분 벡터 $u$의 크기를 1로 고정하는 제약 조건입니다.\n",
    "\n",
    "**라그랑주 승수를 이용한 최적화:**\n",
    "\n",
    "라그랑주 승수법을 사용하여 최적화 문제를 풀기 위한 함수는 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "\\max_u \\left[ u^T x^T x u - \\lambda (u^T u - 1) \\right]\n",
    "$$\n",
    "\n",
    "여기서 $\\lambda$는 라그랑주 승수입니다.\n",
    "\n",
    "**미분하여 최적 조건 찾기:**\n",
    "\n",
    "라그랑주 승수를 포함한 목적 함수를 $u$에 대해 미분하여 0으로 놓습니다:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(u)}{\\partial u} = 0\n",
    "$$\n",
    "\n",
    "**최적화 문제 전개:**\n",
    "\n",
    "$$\n",
    "x^T x u - \\lambda u = 0\n",
    "$$\n",
    "\n",
    "이는 행렬 $x^T x$의 고유값 문제로 변환됩니다:\n",
    "\n",
    "$$\n",
    "x^T x u = \\lambda u\n",
    "$$\n",
    "\n",
    "**주성분 벡터와 고유벡터의 관계:**\n",
    "\n",
    "주성분 벡터 $u$는 행렬 $x^T x$의 고유벡터이며, 대응하는 고유값 $\\lambda$는 주성분의 분산을 나타냅니다. 여기서 $x^T x = \\Sigma$는 공분산 행렬입니다. 따라서:\n",
    "\n",
    "$$\n",
    "\\Sigma u = \\lambda u\n",
    "$$\n",
    "\n",
    "여기서 $u$는 $\\Sigma$의 고유벡터이고, $\\lambda$는 대응하는 고유값입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ec37ab58da66ca9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "라그랑주 승수를 이용한 최적화 과정에서, 주어진 목적 함수와 제약 조건을 합쳐 하나의 라그랑주 함수를 정의한 후, 이를 편미분하여 최적 조건을 찾는 과정을 설명하겠습니다.\n",
    "\n",
    "### 문제 정의\n",
    "\n",
    "우선, 최대화 문제와 제약 조건을 다시 살펴보겠습니다:\n",
    "\n",
    "$$\n",
    "\\max u^T x^T x u \\quad \\text{s.t.} \\quad u^T u = 1\n",
    "$$\n",
    "\n",
    "여기서, $u^T x^T x u$는 데이터의 분산을 $u$ 방향으로 투영한 값이고, $u^T u = 1$은 주성분 벡터의 크기를 1로 고정하는 제약 조건입니다.\n",
    "\n",
    "### 라그랑주 함수\n",
    "\n",
    "위의 최대화 문제를 풀기 위해 라그랑주 승수 $\\lambda$를 도입하고, 다음과 같은 라그랑주 함수를 정의합니다:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(u, \\lambda) = u^T x^T x u - \\lambda (u^T u - 1)\n",
    "$$\n",
    "\n",
    "여기서, $\\lambda$는 제약 조건을 포함하는 승수입니다.\n",
    "\n",
    "### 최적 조건 찾기\n",
    "\n",
    "라그랑주 함수 $\\mathcal{L}(u, \\lambda)$를 $u$에 대해 편미분하여 0으로 놓습니다. 즉, $\\mathcal{L}(u, \\lambda)$의 최적값을 찾기 위해 $\\frac{\\partial \\mathcal{L}}{\\partial u} = 0$ 조건을 구합니다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial u} = \\frac{\\partial}{\\partial u} \\left( u^T x^T x u - \\lambda (u^T u - 1) \\right)\n",
    "$$\n",
    "\n",
    "편미분을 전개하면:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial u} (u^T x^T x u) - \\lambda \\frac{\\partial}{\\partial u} (u^T u) = 0\n",
    "$$\n",
    "\n",
    "먼저, 각 항을 편미분합니다.\n",
    "\n",
    "1. $u^T x^T x u$의 편미분:\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial u} (u^T x^T x u)\n",
    "   $$\n",
    "\n",
    "   여기서, $u^T x^T x u$는 스칼라값이고, 이는 $u$의 이차 형태입니다. 행렬 미분의 규칙에 따라, 이 값의 편미분은 다음과 같습니다:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial u} (u^T x^T x u) = 2 x^T x u\n",
    "   $$\n",
    "\n",
    "   이 결과는 행렬 미분의 기본 규칙에서 유도됩니다.\n",
    "\n",
    "2. $\\lambda (u^T u - 1)$의 편미분:\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial u} (\\lambda (u^T u - 1)) = \\lambda \\frac{\\partial}{\\partial u} (u^T u) = 2\\lambda u\n",
    "   $$\n",
    "\n",
    "이를 라그랑주 함수의 편미분 결과에 적용하면:\n",
    "\n",
    "$$\n",
    "2 x^T x u - 2\\lambda u = 0\n",
    "$$\n",
    "\n",
    "양변을 2로 나누면:\n",
    "\n",
    "$$\n",
    "x^T x u = \\lambda u\n",
    "$$\n",
    "\n",
    "이 결과는 $x^T x u$가 $\\lambda u$로 표현되는 형태로, 이는 $x^T x$의 고유값 문제와 동일합니다. 즉, $u$는 행렬 $x^T x$의 고유벡터이며, $\\lambda$는 대응하는 고유값입니다.\n",
    "\n",
    "### 요약\n",
    "\n",
    "라그랑주 함수를 사용하여 최적화 문제를 풀 때, 제약 조건을 포함한 함수의 편미분을 통해 최적 조건을 찾습니다. 이 과정에서:\n",
    "\n",
    "1. 목적 함수를 라그랑주 함수로 변환하고,\n",
    "2. 이를 변수에 대해 편미분하여,\n",
    "3. 0으로 놓아 최적 조건을 구합니다.\n",
    "\n",
    "이를 통해, $u$는 행렬 $x^T x$의 고유벡터가 되고, $\\lambda$는 대응하는 고유값이 됩니다. 이는 주성분 분석(PCA)에서 주성분 벡터를 찾는 과정과 동일합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b22648dcd7914fab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "주어진 식을 기반으로 행렬 방정식의 기댓값과 분산을 구하는 과정을 전개하겠습니다. 각 단계에서 오류를 수정하고 정확한 수식을 작성하겠습니다.\n",
    "\n",
    "### 모델 정의\n",
    "\n",
    "$$\n",
    "X = \\mu + Lf + \\epsilon\n",
    "$$\n",
    "\n",
    "- $X$: $p$개의 관찰된 변수들의 벡터 $(X_1, X_2, ..., X_p)$\n",
    "- $L$: $p \\times m$ 크기의 요인 적재 행렬(Factor Loadings Matrix), 각 요인이 변수에 미치는 영향력\n",
    "- $f$: $m$개의 공통 요인들(Latent Factors)의 벡터 $(F_1, F_2, ..., F_m)$\n",
    "- $\\epsilon$: 잔차 벡터(Noise), 각 변수의 특성 요인(Specific Factor)\n",
    "\n",
    "### 가정\n",
    "\n",
    "1. $f \\sim N(0, I)$\n",
    "2. $\\epsilon \\sim N(0, \\psi)$\n",
    "\n",
    "여기서, $I$는 $m \\times m$ 단위 행렬이고, $\\psi$는 $p \\times p$ 대각 행렬입니다.\n",
    "\n",
    "### 기댓값 계산\n",
    "\n",
    "$$\n",
    "E(X) = E(\\mu + Lf + \\epsilon)\n",
    "$$\n",
    "\n",
    "기댓값의 선형성에 따라,\n",
    "\n",
    "$$\n",
    "E(X) = E(\\mu) + E(Lf) + E(\\epsilon)\n",
    "$$\n",
    "\n",
    "모든 요인들은 $\\mu$와 독립적이며, $E(f) = 0$ 및 $E(\\epsilon) = 0$이므로,\n",
    "\n",
    "$$\n",
    "E(X) = \\mu + L \\cdot 0 + 0 = \\mu\n",
    "$$\n",
    "\n",
    "따라서,\n",
    "\n",
    "$$\n",
    "E(X) = \\mu\n",
    "$$\n",
    "\n",
    "### 분산 계산\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = E[(X - E(X))(X - E(X))^T]\n",
    "$$\n",
    "\n",
    "여기서 $E(X) = \\mu$임을 사용하면,\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = E[(X - \\mu)(X - \\mu)^T]\n",
    "$$\n",
    "\n",
    "$X$를 $\\mu + Lf + \\epsilon$로 대체하면,\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = E[(\\mu + Lf + \\epsilon - \\mu)(\\mu + Lf + \\epsilon - \\mu)^T]\n",
    "$$\n",
    "\n",
    "이는 다음과 같이 단순화됩니다:\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = E[(Lf + \\epsilon)(Lf + \\epsilon)^T]\n",
    "$$\n",
    "\n",
    "### 공분산 계산\n",
    "\n",
    "이제 $Lf + \\epsilon$의 공분산을 계산합니다:\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = E[(Lf + \\epsilon)(Lf + \\epsilon)^T]\n",
    "$$\n",
    "\n",
    "분산의 선형성을 사용하여 전개합니다:\n",
    "\n",
    "$$\n",
    "= E[Lf f^T L^T + Lf \\epsilon^T + \\epsilon f^T L^T + \\epsilon \\epsilon^T]\n",
    "$$\n",
    "\n",
    "이제 각 항목의 기댓값을 구합니다:\n",
    "\n",
    "1. $E[Lf f^T L^T] = L E[f f^T] L^T = L I L^T = LL^T$ (여기서 $E[f f^T] = I$임을 사용)\n",
    "2. $E[Lf \\epsilon^T] = L E[f \\epsilon^T] = L \\cdot 0 = 0$ (f와 $\\epsilon$은 독립적이므로 $E[f \\epsilon^T] = 0$)\n",
    "3. $E[\\epsilon f^T L^T] = E[\\epsilon f^T] L^T = 0 L^T = 0$ (위와 같은 이유로)\n",
    "4. $E[\\epsilon \\epsilon^T] = \\psi$ (잔차 벡터 $\\epsilon$의 공분산 행렬)\n",
    "\n",
    "따라서,\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = LL^T + \\psi\n",
    "$$\n",
    "\n",
    "### 결론\n",
    "\n",
    "주어진 식과 가정에 따라 $X$의 기댓값과 분산은 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "E(X) = \\mu\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = LL^T + \\psi\n",
    "$$\n",
    "\n",
    "이는 $X$가 다음과 같은 정규분포를 따름을 의미합니다:\n",
    "\n",
    "$$\n",
    "X \\sim N(\\mu, LL^T + \\psi)\n",
    "$$\n",
    "\n",
    "위 과정은 주어진 모델에 대한 정확한 기댓값과 분산을 구하는 과정을 보여줍니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d80399d8abcbcbba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 클러스터 할당 (Cluster Assignment)\n",
    "$$\n",
    "\\gamma_{nk} = \\arg\\min_k \\| x_n - \\mu_k \\|^2\n",
    "$$\n",
    "\n",
    "여기서 $\\gamma_{nk}$는 데이터 포인트 $x_n$이 클러스터 $k$에 속하는지 여부를 나타내는 지표입니다. 이는 다음과 같은 의미를 가집니다:\n",
    "- $\\gamma_{nk} = 1$이면 데이터 포인트 $x_n$이 클러스터 $k$에 속합니다.\n",
    "- $\\gamma_{nk} = 0$이면 데이터 포인트 $x_n$이 클러스터 $k$에 속하지 않습니다.\n",
    "\n",
    "이 수식은 각 데이터 포인트 $x_n$이 가장 가까운 클러스터 중심 $\\mu_k$에 할당되도록 합니다. 즉, 각 데이터 포인트는 자신과의 유클리드 거리가 최소가 되는 클러스터에 할당됩니다.\n",
    "\n",
    "### 2. 클러스터 중심 업데이트 (Centroid Update)\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum \\gamma_{nk} x_n}{\\sum \\gamma_{nk}}\n",
    "$$\n",
    "\n",
    "여기서 $\\mu_k$는 클러스터 $k$의 중심(즉, 평균)을 나타냅니다. 이 수식은 다음과 같은 과정을 통해 클러스터 중심을 업데이트합니다:\n",
    "- 클러스터 $k$에 속하는 모든 데이터 포인트 $x_n$을 더합니다. 이는 $\\gamma_{nk}$가 1인 경우에만 해당합니다.\n",
    "- 그 합을 클러스터 $k$에 속하는 데이터 포인트의 수로 나눕니다.\n",
    "\n",
    "이 과정은 클러스터 중심이 해당 클러스터에 속하는 데이터 포인트의 평균이 되도록 합니다.\n",
    "\n",
    "### 3. K-means 알고리즘 요약\n",
    "\n",
    "K-means 알고리즘은 다음과 같은 단계로 작동합니다:\n",
    "\n",
    "1. **초기화**: $k$개의 클러스터 중심 $\\mu_1, \\mu_2, \\ldots, \\mu_k$를 초기화합니다. 이는 임의의 데이터 포인트를 선택하거나 랜덤하게 설정할 수 있습니다.\n",
    "2. **클러스터 할당 (Assignment Step)**: 각 데이터 포인트 $x_n$을 가장 가까운 클러스터 중심 $\\mu_k$에 할당합니다. 이는 다음과 같은 방식으로 이루어집니다:\n",
    "   $$\n",
    "   \\gamma_{nk} = \\arg\\min_k \\| x_n - \\mu_k \\|^2\n",
    "   $$\n",
    "3. **클러스터 중심 업데이트 (Update Step)**: 각 클러스터 $k$의 중심을 해당 클러스터에 할당된 데이터 포인트의 평균으로 업데이트합니다:\n",
    "   $$\n",
    "   \\mu_k = \\frac{\\sum_{n} \\gamma_{nk} x_n}{\\sum_{n} \\gamma_{nk}}\n",
    "   $$\n",
    "4. **수렴 확인**: 클러스터 할당이 더 이상 변하지 않거나 클러스터 중심의 변화가 매우 작아질 때까지 할당 단계와 업데이트 단계를 반복합니다.\n",
    "\n",
    "### 추가적인 설명\n",
    "\n",
    "#### 초기화 방법\n",
    "K-means 알고리즘의 초기 클러스터 중심 선택은 결과에 큰 영향을 미칠 수 있습니다. 잘못된 초기화는 수렴 속도를 늦추거나 전역 최적해 대신 국소 최적해에 수렴하게 할 수 있습니다. 이를 해결하기 위해 K-means++와 같은 초기화 기법을 사용할 수 있습니다. K-means++는 다음과 같은 단계를 따릅니다:\n",
    "1. 첫 번째 클러스터 중심을 데이터 포인트 중에서 무작위로 선택합니다.\n",
    "2. 나머지 클러스터 중심은 현재 클러스터 중심과의 거리가 먼 데이터 포인트를 우선적으로 선택합니다.\n",
    "\n",
    "#### 알고리즘의 수렴\n",
    "K-means 알고리즘은 유한한 반복 후에 수렴합니다. 이는 각 반복에서 클러스터 할당과 중심 업데이트 단계가 목표 함수 $J$를 감소시키기 때문입니다. 목표 함수 $J$는 데이터 포인트와 클러스터 중심 사이의 거리의 제곱합이므로, 이 값은 항상 0 이상입니다. 따라서 알고리즘은 몇 번의 반복 후에 수렴하게 됩니다.\n",
    "\n",
    "\n",
    "$$\n",
    "J = \\sum_{k=1}^{K} \\sum_{n \\in C_k} \\| x_n - \\mu_k \\|^2\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba3562fedd256697"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 수정된 수식과 설명\n",
    "\n",
    "수식을 제대로 이해하기 위해 각 부분을 살펴보겠습니다.\n",
    "\n",
    "1. **클러스터 중심 $ \\mu_k $의 업데이트 식**:\n",
    "   $$\n",
    "   \\mu_k = \\frac{\\sum_n\\gamma_{nk}x_n}{\\sum_n\\gamma_{nk}}\n",
    "   $$\n",
    "   - $ \\mu_k $: 클러스터 $ k $의 중심(평균).\n",
    "   - $ \\gamma_{nk} $: 데이터 포인트 $ x_n $이 클러스터 $ k $에 속할 확률.\n",
    "   - 위 식은 EM(Expectation-Maximization) 알고리즘에서 E 단계에서 각 클러스터 중심을 업데이트하는 과정을 나타냅니다. 각 데이터 포인트 $ x_n $은 해당 클러스터에 속할 확률 $ \\gamma_{nk} $으로 가중 평균하여 클러스터 중심을 업데이트합니다.\n",
    "\n",
    "2. **클러스터 분산 $ \\gamma_k^2 $의 계산 식**:\n",
    "   $$\n",
    "   \\gamma_k^2 = \\frac{\\sum_n\\gamma_{nk}(x_n-\\mu_k)^2}{\\sum_n\\gamma_{nk}}\n",
    "   $$\n",
    "   - $ \\gamma_k^2 $: 클러스터 $ k $의 분산(혹은 표준 편차의 제곱).\n",
    "   - $ (x_n - \\mu_k)^2 $: 데이터 포인트 $ x_n $과 클러스터 중심 $ \\mu_k $ 간의 거리의 제곱.\n",
    "   - 위 식은 클러스터 $ k $ 내에서 데이터 포인트들이 평균 $ \\mu_k $ 주변에 얼마나 집중되어 있는지를 나타내는 값입니다. 클러스터의 분산을 업데이트하는 데 사용됩니다.\n",
    "\n",
    "3. **전체 확률 $ P(x) $의 계산 식**:\n",
    "   $$\n",
    "   P(x) = \\sum_z P(x|z)p(z) = \\sum_k P(x|z=k)p(z=k)\n",
    "   $$\n",
    "   - $ P(x) $: 주어진 데이터 $ x $의 전체 확률.\n",
    "   - $ P(x|z=k) $: 데이터 $ x $가 클러스터 $ k $에 속할 조건부 확률.\n",
    "   - $ p(z=k) $: 클러스터 $ k $에 속할 사전 확률.\n",
    "   - 위 식은 베이즈 정리를 사용하여 데이터 $ x $의 전체 확률을 계산하는 과정을 나타냅니다. 여기서 $ z $는 클러스터 할당 변수를 나타내며, $ k $는 가능한 클러스터 인덱스를 나타냅니다.\n",
    "\n",
    "4. **사전 확률 $ p(z=k) $의 계산 식**:\n",
    "   $$\n",
    "   p(z=k) = \\pi_k\n",
    "   $$\n",
    "   - $ p(z=k) $: 클러스터 $ k $에 속할 사전 확률.\n",
    "   - $ \\pi_k $는 각 클러스터 $ k $의 가중치(혼합 계수)로, $\\sum_{k=1}^{K} \\pi_k = 1 $입니다. 사전 확률은 각 클러스터가 데이터 포인트에 할당될 초기 확률을 의미합니다.\n",
    "\n",
    "### 설명\n",
    "\n",
    "- **수식 해석**: 각 수식은 $ k $-means나 Gaussian Mixture Model(GMM)과 같은 클러스터링 알고리즘에서 사용되는 중요한 개념들을 나타내며, 데이터를 클러스터로 그룹화하고 클러스터의 중심과 분산을 업데이트하는 과정을 설명합니다.\n",
    "  \n",
    "- **의미**: 클러스터링 알고리즘에서 중심 $ \\mu_k $과 분산 $ \\gamma_k^2 $을 올바르게 업데이트하는 것은 데이터를 적절히 그룹화하고, 각 클러스터의 특성을 정확히 모델링하는 데 중요합니다.\n",
    "  \n",
    "- **수식의 활용**: 이러한 수식들은 EM 알고리즘 등에서 사용되며, 데이터 포인트의 클러스터 소속을 결정하는 데 필수적입니다. 클러스터링 알고리즘이 데이터를 어떻게 그룹화할지 결정하는 과정에서 위 수식들이 핵심적인 역할을 합니다.\n",
    "\n",
    "위의 설명과 수정된 수식들이 클러스터링 알고리즘의 수학적 원리를 잘 나타내고 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e3d74d1aa4923"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 로그 우도 함수\n",
    "\n",
    "GMM의 로그 우도 함수는 주어진 데이터 $ \\{x_n\\}_{n=1}^N $에 대한 모델의 적합도를 측정하는데 사용됩니다. GMM의 확률 밀도 함수는 다음과 같이 정의됩니다:\n",
    "\n",
    "$$\n",
    "p(x|\\theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "- $ \\pi_k $는 클러스터 $ k $의 혼합 계수입니다.\n",
    "- $ \\mathcal{N}(x | \\mu_k, \\Sigma_k) $는 평균 $ \\mu_k $와 공분산 $ \\Sigma_k $를 갖는 정규 분포입니다.\n",
    "\n",
    "데이터의 로그 우도 함수는 다음과 같이 표현됩니다:\n",
    "\n",
    "$$\n",
    "\\log p(x|\\theta) = \\log \\left(\\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\\right)\n",
    "$$\n",
    "\n",
    "여기서 $ \\theta = \\{\\pi_k, \\mu_k, \\Sigma_k\\} $는 모델의 모든 매개변수를 포함합니다.\n",
    "\n",
    "### 로그 우도의 최대화\n",
    "\n",
    "GMM의 파라미터를 추정하기 위해 로그 우도 함수의 최대값을 찾습니다. 로그 우도 함수의 최대화는 일반적으로 다음과 같은 과정으로 이루어집니다:\n",
    "\n",
    "1. **E-단계 (Expectation Step)**:\n",
    "   이 단계에서는 현재의 파라미터 $ \\theta $ 하에서 각 데이터 포인트 $ x_n $이 클러스터 $ k $에 속할 확률, 즉 책임도 $ \\gamma_{nk} = P(z = k | x_n, \\theta) $를 계산합니다.\n",
    "\n",
    "   책임도는 다음과 같이 계산됩니다:\n",
    "   $$\n",
    "   \\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}\n",
    "   $$\n",
    "\n",
    "2. **M-단계 (Maximization Step)**:\n",
    "   이 단계에서는 E-단계에서 계산된 책임도를 사용하여 모델의 파라미터를 업데이트합니다. 업데이트되는 파라미터는 다음과 같습니다:\n",
    "\n",
    "   - **혼합 계수** $ \\pi_k $:\n",
    "     $$\n",
    "     \\pi_k = \\frac{1}{N} \\sum_{n=1}^N \\gamma_{nk}\n",
    "     $$\n",
    "\n",
    "   - **클러스터 평균** $ \\mu_k $:\n",
    "     $$\n",
    "     \\mu_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} x_n}{\\sum_{n=1}^N \\gamma_{nk}}\n",
    "     $$\n",
    "\n",
    "   - **클러스터 공분산** $ \\Sigma_k $:\n",
    "     $$\n",
    "     \\Sigma_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} (x_n - \\mu_k)(x_n - \\mu_k)^T}{\\sum_{n=1}^N \\gamma_{nk}}\n",
    "     $$\n",
    "\n",
    "### 로그 우도의 최대화 목적\n",
    "\n",
    "로그 우도를 최대화하는 목적은 다음과 같습니다:\n",
    "\n",
    "1. **최대 로그 우도 추정**:\n",
    "   로그 우도를 최대화하는 파라미터 $ \\theta $를 찾습니다. 이는 다음과 같은 수식으로 표현됩니다:\n",
    "   $$\n",
    "   \\theta^{*} = \\arg\\max_{\\theta} \\sum_{n=1}^N \\log p(x_n | \\theta)\n",
    "   $$\n",
    "\n",
    "2. **EM 알고리즘의 수렴**:\n",
    "   E-단계와 M-단계를 반복하면서 로그 우도가 증가하는 방향으로 파라미터를 업데이트합니다. 알고리즘은 로그 우도가 수렴할 때까지 반복됩니다.\n",
    "\n",
    "### 수식 정리\n",
    "\n",
    "1. **로그 우도 함수**:\n",
    "   $$\n",
    "   \\log p(x|\\theta) = \\log \\left(\\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\\right)\n",
    "   $$\n",
    "\n",
    "2. **E-단계 책임도 계산**:\n",
    "   $$\n",
    "   \\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}\n",
    "   $$\n",
    "\n",
    "3. **M-단계 파라미터 업데이트**:\n",
    "   - 혼합 계수:\n",
    "     $$\n",
    "     \\pi_k = \\frac{1}{N} \\sum_{n=1}^N \\gamma_{nk}\n",
    "     $$\n",
    "   - 평균:\n",
    "     $$\n",
    "     \\mu_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} x_n}{\\sum_{n=1}^N \\gamma_{nk}}\n",
    "     $$\n",
    "   - 공분산:\n",
    "     $$\n",
    "     \\Sigma_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} (x_n - \\mu_k)(x_n - \\mu_k)^T}{\\sum_{n=1}^N \\gamma_{nk}}\n",
    "     $$\n",
    "\n",
    "이 과정에서 EM 알고리즘의 반복적인 업데이트를 통해 GMM의 파라미터가 최적화됩니다. 이 과정을 통해 데이터의 로그 우도를 최대화할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3743ea25b743d830"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 수정된 수식\n",
    "\n",
    "1. **정규분포 확률 밀도 함수**:\n",
    "   $$\n",
    "   f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "   $$\n",
    "\n",
    "2. **로그 우도 함수**:\n",
    "   $$\n",
    "   L(\\mu,\\sigma^2) = \\log\\prod_{i=1}^n f(x_i|\\mu,\\sigma^2) = \\sum_{i=1}^n \\log f(x_i|\\mu,\\sigma^2)\n",
    "   $$\n",
    "\n",
    "3. **로그 우도 함수 전개**:\n",
    "   $$\n",
    "   L(\\mu,\\sigma^2) = \\sum_{i=1}^n \\left[ -\\log \\sqrt{2\\pi} - \\log \\sigma - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right]\n",
    "   $$\n",
    "\n",
    "4. **로그 우도 함수의 $\\mu$에 대한 편미분**:\n",
    "   $$\n",
    "   \\frac{\\partial L(\\mu, \\sigma^2)}{\\partial \\mu} = \\sum_{i=1}^n \\left( \\frac{(x_i - \\mu)}{\\sigma^2} \\right)\n",
    "   $$\n",
    "\n",
    "### 수식에 대한 설명\n",
    "\n",
    "1. **정규분포 확률 밀도 함수**:\n",
    "   $$\n",
    "   f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "   $$\n",
    "   - $ f(x|\\mu, \\sigma^2) $는 평균이 $ \\mu $이고 분산이 $ \\sigma^2 $인 정규분포의 확률 밀도 함수입니다.\n",
    "   - $ \\frac{1}{\\sqrt{2\\pi}\\sigma} $는 정규분포의 정규화 상수로, 확률 밀도 함수가 적분 시 1이 되도록 합니다.\n",
    "   - $ \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) $는 $ x $가 평균 $ \\mu $ 주위에 얼마나 집중되어 있는지를 나타냅니다.\n",
    "\n",
    "2. **로그 우도 함수**:\n",
    "   $$\n",
    "   L(\\mu,\\sigma^2) = \\log\\prod_{i=1}^n f(x_i|\\mu,\\sigma^2) = \\sum_{i=1}^n \\log f(x_i|\\mu,\\sigma^2)\n",
    "   $$\n",
    "   - $ L(\\mu, \\sigma^2) $는 주어진 데이터 $ \\{x_i\\}_{i=1}^n $에 대한 로그 우도 함수입니다.\n",
    "   - 각 데이터 포인트 $ x_i $가 정규분포 $ f(x_i|\\mu, \\sigma^2) $에 따라 독립적으로 분포한다고 가정합니다.\n",
    "   - 로그 우도 함수는 개별 확률 밀도의 곱을 로그로 변환하여 합으로 표현한 것입니다.\n",
    "\n",
    "3. **로그 우도 함수 전개**:\n",
    "   $$\n",
    "   L(\\mu,\\sigma^2) = \\sum_{i=1}^n \\left[ -\\log \\sqrt{2\\pi} - \\log \\sigma - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right]\n",
    "   $$\n",
    "   - 각 데이터 포인트 $ x_i $에 대해 로그 우도 함수를 전개한 것입니다.\n",
    "   - $ -\\log \\sqrt{2\\pi} $와 $ -\\log \\sigma $는 정규분포의 정규화 상수 부분입니다.\n",
    "   - $ -\\frac{(x_i - \\mu)^2}{2\\sigma^2} $는 데이터 포인트 $ x_i $와 평균 $ \\mu $ 간의 거리 제곱에 비례하는 항입니다.\n",
    "\n",
    "4. **로그 우도 함수의 $\\mu$에 대한 편미분**:\n",
    "   $$\n",
    "   \\frac{\\partial L(\\mu, \\sigma^2)}{\\partial \\mu} = \\sum_{i=1}^n \\left( \\frac{(x_i - \\mu)}{\\sigma^2} \\right)\n",
    "   $$\n",
    "   - 로그 우도 함수를 평균 $ \\mu $에 대해 편미분한 것입니다.\n",
    "   - 이 편미분은 로그 우도 함수가 최대가 되는 지점을 찾기 위해 사용됩니다.\n",
    "   - 편미분 결과는 각 데이터 포인트 $ x_i $에서 평균 $ \\mu $를 뺀 값의 합에 비례합니다.\n",
    "\n",
    "이를 통해 우리는 정규분포의 로그 우도 함수를 최대화하는 파라미터 $ \\mu $와 $ \\sigma^2 $를 추정할 수 있습니다. 최대 우도 추정법(MLE)은 이 로그 우도 함수를 최대화하는 파라미터 값을 찾는 방법입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47e1fae1d9c7a421"
  },
  {
   "cell_type": "markdown",
   "source": [
    "수식에서 몇 가지 오류가 있습니다. 이를 수정하고, 수식과 Jensen's inequality에 대해 자세히 설명하겠습니다.\n",
    "\n",
    "### 수정된 수식\n",
    "\n",
    "수식을 올바르게 정리하면 다음과 같습니다:\n",
    "\n",
    "1. **로그 우도 함수의 정의**:\n",
    "   $$\n",
    "   \\log p(x;\\theta) = \\log \\sum_z p(x,z;\\theta)\n",
    "   $$\n",
    "\n",
    "2. **결합 확률 분포의 분해**:\n",
    "   $$\n",
    "   = \\log \\sum_z p(x|z;\\theta) p(z;\\theta)\n",
    "   $$\n",
    "\n",
    "3. **기대값 표현**:\n",
    "   $$\n",
    "   = \\log \\sum_z p(z|\\theta) p(x|z;\\theta)\n",
    "   $$\n",
    "\n",
    "4. **Jensen's inequality 적용**:\n",
    "   $$\n",
    "   = \\log E_{p(z|\\theta)}[p(x|z;\\theta)] \\quad \\text{(Jensen's inequality)}\n",
    "   $$\n",
    "\n",
    "5. **Jensen's inequality에 따른 불평등**:\n",
    "   $$\n",
    "   \\geq E_{p(z|\\theta)}[\\log p(x|z;\\theta)] \\quad \\text{(Jensen's inequality 적용)}\n",
    "   $$\n",
    "\n",
    "### 설명\n",
    "\n",
    "#### 1. **로그 우도 함수의 정의**\n",
    "\n",
    "주어진 데이터 $x$의 로그 우도 함수는 다음과 같이 정의됩니다:\n",
    "$$\n",
    "\\log p(x;\\theta) = \\log \\sum_z p(x,z;\\theta)\n",
    "$$\n",
    "여기서 $z$는 잠재 변수(latent variable)이고, $\\theta$는 모델의 파라미터입니다.\n",
    "\n",
    "#### 2. **결합 확률 분포의 분해**\n",
    "\n",
    "결합 확률 $p(x,z;\\theta)$를 조건부 확률과 사전 확률로 분해할 수 있습니다:\n",
    "$$\n",
    "\\log \\sum_z p(x|z;\\theta) p(z;\\theta)\n",
    "$$\n",
    "이는 $x$가 $z$를 조건으로 한 확률 $p(x|z;\\theta)$와 $z$의 사전 확률 $p(z;\\theta)$의 곱의 합입니다.\n",
    "\n",
    "#### 3. **기대값 표현**\n",
    "\n",
    "이제 우리는 사전 확률을 사용하여 기대값의 형태로 재구성할 수 있습니다:\n",
    "$$\n",
    "\\log \\sum_z p(z|\\theta) p(x|z;\\theta)\n",
    "$$\n",
    "\n",
    "#### 4. **Jensen's inequality 적용**\n",
    "\n",
    "Jensen's inequality를 사용하기 위해, 우리는 로그 함수의 속성을 이용합니다. Jensen's inequality에 따르면, 볼록 함수의 기대값의 로그는 로그 기대값보다 큽니다:\n",
    "$$\n",
    "\\log E_{p(z|\\theta)}[p(x|z;\\theta)]\n",
    "$$\n",
    "\n",
    "#### 5. **Jensen's inequality에 따른 불평등**\n",
    "\n",
    "Jensen's inequality를 적용하면 다음과 같은 불평등이 성립합니다:\n",
    "$$\n",
    "\\log E_{p(z|\\theta)}[p(x|z;\\theta)] \\geq E_{p(z|\\theta)}[\\log p(x|z;\\theta)]\n",
    "$$\n",
    "\n",
    "여기서 $p(z|x)$는 $z$가 주어진 $x$에 대한 사후 확률입니다. 이를 통해 우리는 다음과 같이 정리할 수 있습니다:\n",
    "$$\n",
    "\\log p(x;\\theta) \\geq E_{p(z|x)}[\\log p(x|z;\\theta)]\n",
    "$$\n",
    "\n",
    "### Jensen's Inequality\n",
    "\n",
    "Jensen's inequality는 볼록 함수(convex function)와 확률론에서 자주 사용되는 중요한 불평등입니다. 이를 수식으로 표현하면 다음과 같습니다:\n",
    "\n",
    "#### Jensen's Inequality\n",
    "\n",
    "볼록 함수 $f$와 임의의 확률 변수 $X$에 대해:\n",
    "$$\n",
    "f(E[X]) \\leq E[f(X)]\n",
    "$$\n",
    "\n",
    "이를 적용하여 로그 함수의 경우, 로그는 볼록 함수이므로:\n",
    "$$\n",
    "\\log E[X] \\leq E[\\log X]\n",
    "$$\n",
    "\n",
    "따라서, 위의 수식에서 Jensen's inequality를 적용하여:\n",
    "$$\n",
    "\\log \\sum_z p(z|\\theta) p(x|z;\\theta) \\geq \\sum_z p(z|\\theta) \\log p(x|z;\\theta)\n",
    "$$\n",
    "\n",
    "즉, 로그의 기대값은 기대값의 로그보다 작거나 같습니다. 이 불평등은 특히 확률 분포와 최대우도 추정(Maximum Likelihood Estimation, MLE)에서 중요한 역할을 합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ea2558c113b4800"
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\text{E-step} \\\\\n",
    "q(z) = p(z|x) \\\\\n",
    "\\\\\n",
    "\\text{M-step} \\\\\n",
    "\\arg \\max_\\theta E_z[\\log p(x|z;\\theta) \\\\\n",
    "\\\\\n",
    "p(x) = \\sum z_i p(x_i | \\mu_i, \\sum_i) \\\\\n",
    "\\text{E-step} \\\\\n",
    "p(z_i|x_i) = \\frac{p(z_i = k) p(x_i|z_i, \\mu_i, \\sum_i)}{\\sum p(z_i = k) p(x_i | z_i, \\mu_i, \\sum_i)} \\\\\n",
    "\\\\\n",
    "\\text{M-ste} \\\\\n",
    "\\arg \\max_{\\pi, \\mu, \\sum} p(x|\\pi, \\mu, \\sum)\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d1cab9a617cb55b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
